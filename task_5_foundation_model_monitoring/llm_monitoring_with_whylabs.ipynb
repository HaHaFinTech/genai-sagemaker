{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "902132b3-f693-41ab-9455-0966edadec42",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFDDDD; border-left: 5px solid red; padding: 10px; color: black;\">\n",
    "    <strong>Kernel:</strong> Python 3 (ipykernel)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dae933-0b5f-4c6f-b9fc-145e467071b9",
   "metadata": {},
   "source": [
    "# Monitoring Large Language Models (LLMs) with WhyLabs LangKit\n",
    "\n",
    "LangKit is an open-source text metrics toolkit for monitoring language models. It offers an array of methods for extracting relevant signals from the input and/or output text, which are compatible with the open-source data logging library whylogs.\n",
    "\n",
    "In this example we'll show how to generate out-of-the-box metrics for monitoring LLMs using LangKit and visualize them in the WhyLabs Observability Platform.\n",
    "\n",
    "- [Text Quality](https://github.com/whylabs/langkit/blob/main/langkit/docs/features/quality.md)\n",
    "- [Text Relevance](https://github.com/whylabs/langkit/blob/main/langkit/docs/features/relevance.md)\n",
    "- [Security and Privacy](https://github.com/whylabs/langkit/blob/main/langkit/docs/features/security.md)\n",
    "- [Sentiment and Toxicity](https://github.com/whylabs/langkit/blob/main/langkit/docs/features/sentiment.md)\n",
    "\n",
    "\n",
    "![](https://github.com/whylabs/langkit/blob/main/static/img/LangKit_graphic.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa600089-3179-4443-8227-48b9b3f56f16",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aaaf7d-da6a-40d5-81ce-582a84f7f5a0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install the fmeval package\n",
    "%pip install -U datasets==2.21.0\n",
    "%pip install -U jsonlines==4.0.0\n",
    "%pip install -U fmeval==1.2.0\n",
    "%pip install -U py7zr==0.22.0\n",
    "%pip install 'langkit[all]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9371f82b-2722-4b67-a4c1-fcdf6783cccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#temp fix for a bug in SM Studio with ffspec-2023 not being properly updated\n",
    "export SITE_PACKAGES_FOLDER=$(python3 -c \"import sysconfig; print(sysconfig.get_paths()['purelib'])\")\n",
    "rm -rf $SITE_PACKAGES_FOLDER/fsspec-2023*\n",
    "\n",
    "echo \"ffspec-2023 bug fix run successfully\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334d6239-40d0-4c39-9ce9-89ae15f84ef0",
   "metadata": {},
   "source": [
    "## 👋 Hello, World! Take a quick look at LangKit metrics\n",
    "\n",
    "In the below code we log a few example prompt/response pairs and send metrics to WhyLabs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d7551e-1b39-498e-9393-5ddb865574c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langkit import llm_metrics # alternatively use 'light_metrics'\n",
    "import whylogs as why\n",
    "\n",
    "# Note: llm_metrics.init() downloads models so this is slow first time.\n",
    "schema = llm_metrics.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6545e893-167f-4c90-8fad-d815e5c0eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langkit.whylogs.samples import load_chats, show_first_chat\n",
    "\n",
    "# Let's look at what's in this toy example:\n",
    "chats = load_chats()\n",
    "print(f\"There are {len(chats)} records in this toy example data, here's the first one:\")\n",
    "show_first_chat(chats)\n",
    "\n",
    "results = why.log(chats, name=\"langkit-sample-chats-all\", schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e237c5a-01b6-44da-a517-40b560cea9be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "profview = results.view()\n",
    "profview.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc230338-a6a3-4175-b606-e9d18621e4bc",
   "metadata": {},
   "source": [
    "# Text Quality\n",
    "Text quality metrics, such as readability, complexity and grade level, can provide important insights into the quality and appropriateness of generated responses. By monitoring these metrics, we can ensure that the Language Model (LLM) outputs are clear, concise, and suitable for the intended audience.\n",
    "\n",
    "Assessing text complexity and grade level assists in tailoring the generated content to the target audience. By considering factors such as sentence structure, vocabulary choice, and domain-specific requirements, we can ensure that the LLM produces responses that align with the intended reading level and professional context. Additionally, incorporating metrics such as syllable count, word count, and character count allows us to closely monitor the length and composition of the generated text. By setting appropriate limits and guidelines, we can ensure that the responses remain concise, focused, and easily digestible for users.\n",
    "\n",
    "In langkit, we can compute text quality metrics through the textstat module, which uses the textstat library to compute several different text quality metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cf78d3-f1d8-482d-80c4-501c55fd1a02",
   "metadata": {},
   "source": [
    "## flesch_reading_ease\n",
    "This method returns the Flesch Reading Ease score of the input text. The score is based on sentence length and word length. Higher scores indicate material that is easier to read; lower numbers mark passages that are more complex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0598a92-8000-4f55-8c00-8f9ca18db39b",
   "metadata": {},
   "source": [
    "## automated_readability_index\n",
    "\n",
    "This method returns the Automated Readability Index (ARI) of the input text. ARI is a readability test for English texts that estimates the years of schooling a person needs to understand the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7267d86c-39ac-40b5-9a19-07b0f76838c4",
   "metadata": {},
   "source": [
    "## aggregate_reading_level\n",
    "This method returns the aggregate reading level of the input text as calculated by the textstat library, and includes the metrics above denotes with *\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b67901-ba97-4264-bcb9-8dbfb9f1d900",
   "metadata": {},
   "source": [
    "# Text relevance\n",
    "\n",
    "Text relevance plays a crucial role in the monitoring of Language Models (LLMs) by providing an objective measure of the similarity between different texts. It serves multiple use cases, including assessing the quality and appropriateness of LLM outputs and providing guardrails to ensure the generation of safe and desired responses.\n",
    "\n",
    "One use case is computing similarity scores between embeddings generated from prompts and responses, enabling the evaluation of the relevance between them. This helps identify potential issues such as irrelevant or off-topic responses, ensuring that LLM outputs align closely with the intended context. In langkit, we can compute similarity scores between prompt and response pairs using the input_output module.\n",
    "\n",
    "Another use case is calculating the similarity of prompts and responses against certain topics or known examples, such as jailbreaks or controversial subjects. By comparing the embeddings to these predefined themes, we can establish guardrails to detect potential dangerous or unwanted responses. The similarity scores serve as signals, alerting us to content that may require closer scrutiny or mitigation. In langkit, this can be done through the themes module.\n",
    "\n",
    "By leveraging text relevance as a monitoring metric for LLMs, we can not only evaluate the quality of generated responses but also establish guardrails to minimize the risk of generating inappropriate or harmful content. This approach enhances the performance, safety, and reliability of LLMs in various applications, providing a valuable tool for responsible AI development."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c60eba-cbc9-424f-b724-3bee557b83d6",
   "metadata": {},
   "source": [
    "## response.relevance_to_prompt\n",
    "\n",
    "The response.relevance_to_prompt computed column will contain a similarity score between the prompt and response. The higher the score, the more relevant the response is to the prompt.\n",
    "\n",
    "The similarity score is computed by calculating the cosine similarity between embeddings generated from both prompt and response. The embeddings are generated using the hugginface's model sentence-transformers/all-MiniLM-L6-v2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc353581-5430-46ab-bbf7-e0d8bf4a058c",
   "metadata": {},
   "source": [
    "## response.refusal_similarity\t\n",
    "\n",
    "This group gathers a set of known LLM refusal examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b3b8ae-2e4b-4ba7-90fd-53b38342b222",
   "metadata": {},
   "source": [
    "# Security and Privacy\n",
    "Monitoring for security and privacy in Language Model (LLM) applications helps ensuring the protection of user data and preventing malicious activities. Several approaches can be employed to strengthen the security and privacy measures within LLM systems.\n",
    "\n",
    "One approach is to measure text similarity between prompts and responses against known examples of jailbreak attempts, prompt injections, and LLM refusals of service. By comparing the embeddings generated from the text, potential security vulnerabilities and unauthorized access attempts can be identified. This helps in mitigating risks and contributes to the LLM operation within secure boundaries. In langkit, text similarity calculation between prompts/responses and known examples of jailbreak attempts, prompt injections, and LLM refusals of service can be done through the themes module.\n",
    "\n",
    "Having a prompt injection classifier in place further enhances the security of LLM applications. By detecting and preventing prompt injection attacks, where malicious code or unintended instructions are injected into the prompt, the system can maintain its integrity and protect against unauthorized actions or data leaks. In langkit, prompt injection detection metrics can be computed through the injections module and proactive_injection_detection module.\n",
    "\n",
    "LLMs are known for their ability to generate non-factual or nonsensical statements, more commonly known as “hallucinations.” This characteristic can undermine trust in many scenarios where factuality is required, such as summarization tasks, generative question answering, and dialogue generations. In langkit, hallucination detection metrics can be computed through the hallucination module.\n",
    "\n",
    "Another important aspect of security and privacy monitoring involves checking prompts and responses against regex patterns designed to detect sensitive information. These patterns can help identify and flag data such as credit card numbers, telephone numbers, or other types of personally identifiable information (PII). In langkit, regex pattern matching against pattern groups can be done through the regexes module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d2a5da-f6d0-449f-b903-6d7bea824a77",
   "metadata": {},
   "source": [
    "## prompt.jailbreak_similarity\n",
    "This group gathers a set of known jailbreak examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27753f2f-0914-425c-a9a6-d4a42ef80735",
   "metadata": {},
   "source": [
    "## prompt.injection\n",
    "The prompt.injection column will return the maximum similarity score between the target and a group of known jailbreak attempts and harmful behaviors, which is stored as a vector db using the FAISS package. The higher the score, the more similar it is to a known jailbreak attempt or harmful behavior.\n",
    "\n",
    "This metric is similar to the jailbreak_similarity from themes module. The difference is that the injection module will compute similarity against a much larger set of examples, but the used encoder and set of examples are not customizable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369bd8cb-8bc6-416a-9866-61b36e0dd1d7",
   "metadata": {},
   "source": [
    "## has_patterns\n",
    "Each value in the string column will be searched by the regexes patterns in pattern_groups.json. If any pattern within a certain group matches, the name of the group will be returned while generating the has_patterns submetric. For instance, if any pattern in the mailing_adress is a match, the value mailing_address will be returned.\n",
    "\n",
    "The regexes are applied in the order defined in pattern_groups.json. If a value matches multiple patterns, the first pattern that matches will be returned, so the order of the groups in pattern_groups.json is important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee84800b-00ac-4f4f-809a-63f4bb4f1293",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "The use of sentiment analysis for monitoring Language Model (LLM) applications can provide valuable insights into the appropriateness and user engagement of generated responses. By employing sentiment and toxicity classifiers, we can assess the sentiment and detect potentially harmful or inappropriate content within LLM outputs.\n",
    "\n",
    "Monitoring sentiment allows us to gauge the overall tone and emotional impact of the responses. By analyzing sentiment scores, we can ensure that the LLM is consistently generating appropriate and contextually relevant responses. For instance, in customer service applications, maintaining a positive sentiment ensures a satisfactory user experience.\n",
    "\n",
    "Additionally, toxicity analysis provides an important measure of the presence of offensive, disrespectful, or harmful language in LLM outputs. By monitoring toxicity scores, we can identify potentially inappropriate content and take necessary actions to mitigate any negative impact.\n",
    "\n",
    "Analyzing sentiment and toxicity scores in LLM applications also serves other motivations. It enables us to identify potential biases or controversial opinions present in the responses, helping to address concerns related to fairness, inclusivity, and ethical considerations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbed444-dd85-4a22-845c-15d26e07e6b9",
   "metadata": {},
   "source": [
    "## sentiment_nltk\n",
    "The sentiment_nltk will contain metrics related to the compound sentiment score calculated for each value in the string column. The sentiment score is calculated using nltk's Vader sentiment analyzer. The score ranges from -1 to 1, where -1 is the most negative sentiment and 1 is the most positive sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e7e186-ce78-43f5-b0a6-5aa20923fef9",
   "metadata": {},
   "source": [
    "## toxicity\n",
    "\n",
    "The toxicity will contain metrics related to the toxicity score calculated for each value in the string column. By default, the toxicity score is calculated using HuggingFace's martin-ha/toxic-comment-model toxicity analyzer. The score ranges from 0 to 1, where 0 is no toxicity and 1 is maximum toxicity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90353819-580b-4b53-b0e8-c9eb7c4f9733",
   "metadata": {},
   "source": [
    "# Use LangKit to monitor Meta-Llama-3.1-8B-Instruct model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594b214a-aedc-4b4c-bf4c-37fd048c2b58",
   "metadata": {},
   "source": [
    "Here you will use the HuggingFace datasets package to load the Samsum dataset. The dataset is pre-split into training and test data, so you can simply take that split using the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f920ab-e925-4b96-9cbc-50b36b6af3fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "test_dataset  = load_dataset(\"Samsung/samsum\", split=\"test\")\n",
    "\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e28999-5260-4b99-94e7-37ac6d2e6161",
   "metadata": {},
   "source": [
    "You can see the test dataset has 819 items in it, and they can be accessed via index. The items include the transcription of the earnings call and a short summary of that dialogue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534a28c3-889c-4375-9b89-544fee91093d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset[204]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dd0207-0d93-490c-bc07-21d96afd31d9",
   "metadata": {},
   "source": [
    "Create the client objects for calling SageMaker APIs, and supply the names of the SageMaker endpoints you created for the base and fine-tuned versions of the model. If you did not deploy both models, you can simply set them to the same endpoint name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc561a07-56ab-40ed-ac88-faa76c73eef7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "boto_session = boto3.session.Session()\n",
    "region = boto_session.region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e9647d-f1a4-49e2-ad22-8ca13de17937",
   "metadata": {},
   "source": [
    "## Enter your endpoints here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdef9927-14cb-4b31-bef8-12c8509dba6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_endpoint_name = \"meta-llama31-8b-instruct-endpoint\"  #replace with yours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd23fe0-49d0-4b10-aa07-90309913a1b0",
   "metadata": {},
   "source": [
    "### As a quick test, you will take a base prompt and sample from the dataset to verify that the endpoints provided will work for the upcoming test runs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b74741-ac5e-40f2-8e96-eb8fe9022322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "prompt = f\"\"\"\n",
    "<|begin_of_text|>\n",
    "<|start_header_id|>system<|end_header_id|>\n",
    "You are a helpful assistant who is an expert in summarizing conversations.\n",
    "<|eot_id|>\n",
    "\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "Summarize the provided conversation in 2 sentences.\n",
    "\n",
    "{test_dataset[0]['dialogue']}\n",
    "\n",
    "Provide the summary directly, without any introduction or preamble. Do not start the response with \"Here is a...\".<|eot_id|>\n",
    "\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "base_payload = {\"inputs\": prompt,\"parameters\": {\"do_sample\": True,\"top_p\": 0.9,\"temperature\": 0.8,\"max_new_tokens\": 256,},}\n",
    "\n",
    "\n",
    "base_predictor = sagemaker.Predictor(\n",
    "    endpoint_name = base_endpoint_name,\n",
    "    sagemaker_session = sess,\n",
    "    serializer = sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer = sagemaker.deserializers.JSONDeserializer(),\n",
    ")\n",
    "\n",
    "base_predictor_response = base_predictor.predict(base_payload)\n",
    "\n",
    "print(f\"Base Model:\\n{base_predictor_response['generated_text']}\")\n",
    "print(\"\\n ================ \\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e114d7e6-cd69-449a-993e-c316c73eeb6e",
   "metadata": {},
   "source": [
    "## ML Monitoring for LLMs in WhyLabs\n",
    "\n",
    "\n",
    "To send LangKit profiles to WhyLabs we will need three pieces of information:\n",
    "\n",
    "- API token\n",
    "- Organization ID\n",
    "- Dataset ID (or model-id)\n",
    "\n",
    "Go to [https://whylabs.ai/free](https://whylabs.ai/free) and grab a free account. You can follow along with the quick start examples or skip them if you'd like to follow this example immediately.\n",
    "\n",
    "1. Create a new project and note its ID (if it's a model project, it will look like `model-xxxx`)\n",
    "2. Create an API token from the \"Access Tokens\" tab\n",
    "3. Copy your org ID from the same \"Access Tokens\" tab\n",
    "\n",
    "Replace the placeholder string values with your WhyLabs API Keys below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac164d80-3b99-46f1-8916-7011e8c76385",
   "metadata": {},
   "source": [
    "![](./1_access-token.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803ede84-1107-4b98-8da0-56d396a93e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import whylogs as why\n",
    "\n",
    "#os.environ[\"WHYLABS_DEFAULT_ORG_ID\"] = \"xxx\" # ORG-ID is case sensitive\n",
    "#os.environ[\"WHYLABS_API_KEY\"] = \"xxx\" # API-KEY\n",
    "#os.environ[\"WHYLABS_DEFAULT_DATASET_ID\"] = \"xxx\" # MODEL-ID\n",
    "\n",
    "os.environ[\"WHYLABS_DEFAULT_ORG_ID\"] = \"org-segrKk\" # ORG-ID is case sensitive\n",
    "os.environ[\"WHYLABS_API_KEY\"] = \"pXeBXBtIm8.vLXWBjiVlvL3orjBRtJsQ6JJ91lD5jDTKgJreRAFclAXrCLHFSxq2:org-segrKk\"\n",
    "os.environ[\"WHYLABS_DEFAULT_DATASET_ID\"] = \"model-2\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f183cc-3826-43f4-aceb-56f3731f2a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from whylogs.api.writer.whylabs import WhyLabsWriter\n",
    "from langkit import llm_metrics # alternatively use 'light_metrics'\n",
    "import whylogs as why\n",
    "\n",
    "\n",
    "# Set to show all columns in dataframe\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# Note: llm_metrics.init() downloads models so this is slow first time.\n",
    "schema = llm_metrics.init()\n",
    "why.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199c518d-32a3-496c-987c-a6c90447a910",
   "metadata": {},
   "source": [
    "### Create & Inspect  Language Metrics with LangKit\n",
    "\n",
    "LangKit provides a toolkit of metrics for LLM applications, lets initialize them and create a profile of the data that can be viewed in WhyLabs for quick analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddd33ac-ea1e-4830-a7d3-3df68a4e3cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_prompt = f\"\"\"\n",
    "    <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "    {{question}}\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172e336b-9549-4a67-944b-2b0a30a02d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fn(prompt, question):\n",
    "    \n",
    "    response = base_predictor.predict({\n",
    "\t\"inputs\": prompt,\n",
    "    \"parameters\": {\n",
    "        \"n_predict\": -1,\n",
    "        \"temperature\": 0.2,\n",
    "        \"top_p\": 0.9,\n",
    "        \"stop\": [\"<|start_header_id|>\", \"<|eot_id|>\", \"<|start_header_id|>user<|end_header_id|>\", \"assistant\"]\n",
    "    }\n",
    "    })\n",
    "\n",
    "    #print(response)\n",
    "\n",
    "    # parse response text\n",
    "    resp = response['generated_text']\n",
    "    \n",
    "    rhi = resp.rfind('<|end_header_id|>')\n",
    "    rhi = rhi + 1\n",
    "    \n",
    "    resp_mod = resp[rhi:]\n",
    "\n",
    "    # remove \\t\\n\n",
    "    resp_mod = resp_mod.replace('\\n', '')\n",
    "    resp_mod = resp_mod.replace('\\t', '')\n",
    "\n",
    "    question_mod = question.replace('\\n', '')\n",
    "    question_mod = question_mod.replace('\\t', '')\n",
    "    question_mod = question_mod.replace('\\r', '')\n",
    "\n",
    "\n",
    "    prompt_and_response = {\n",
    "      \"prompt\": question_mod,\n",
    "      \"response\": resp_mod\n",
    "    }\n",
    "    \n",
    "    return prompt_and_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c74b15-1b3b-4aa6-b955-3cc2b9b6328c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"What is the context window of Anthropic Claude 2.1 model?\"\n",
    "prompt = base_prompt.format(question=question)\n",
    "\n",
    "prompt_and_response = predict_fn(prompt,question)\n",
    "print(prompt_and_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f451b82-5491-472d-a0c6-d17f96f01b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = f\"\"\"\n",
    "<|begin_of_text|>\n",
    "<|start_header_id|>system<|end_header_id|>\n",
    "You are a helpful assistant who is an expert in summarizing conversations.\n",
    "<|eot_id|>\n",
    "\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "Summarize the provided conversation in 2 sentences.\n",
    "\n",
    "{{question}}\n",
    "\n",
    "Provide the summary directly, without any introduction or preamble. Do not start the response with \"Here is a...\".<|eot_id|>\n",
    "\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8797f92-6b05-45a7-818f-070a1aaf2be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "question=test_dataset[0]['dialogue']\n",
    "prompt = prompt_template.format(question=question)\n",
    "\n",
    "prompt_and_response = predict_fn(prompt,question)\n",
    "print(prompt_and_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab1c905-1ee5-419e-a2fc-21d043ce05cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = why.log(prompt_and_response, schema=schema).profile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf3a3b8-f943-4013-9b41-d10904d5ad37",
   "metadata": {},
   "outputs": [],
   "source": [
    "profview = profile.view()\n",
    "profview.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3f6c33-5026-4f9a-8c0d-ea426ec276cb",
   "metadata": {},
   "source": [
    "## Batch Monitor LLM prompt and response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8410c49-78e2-4b43-9371-7662461b1daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = why.log(prompt_and_response, schema=schema).profile()\n",
    "profview = profile.view()\n",
    "profview.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c187a6f-20b0-459e-ac63-03fbda2de299",
   "metadata": {},
   "source": [
    "## Batch Monitor LLM prompt and response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef4ac35-7cac-4f1f-8075-c6b2c9440fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 10\n",
    "pr_list = []\n",
    "for i in range(0,x):\n",
    "    question=test_dataset[i]['dialogue']\n",
    "    prompt = prompt_template.format(question=question)\n",
    "    prompt_and_response = predict_fn(prompt,question)\n",
    "    pr_list.append(prompt_and_response)\n",
    "\n",
    "df_pr = pd.DataFrame(pr_list)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c19841-bd8a-4d7a-a902-afe546dea5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = why.log(df_pr, name=\"sam-sum-list\", schema=schema).profile()\n",
    "profview = profile.view()\n",
    "profview.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2983fc5-87b2-49f8-b408-586df3539baa",
   "metadata": {},
   "source": [
    "### Back Filling with WhyLabs\n",
    "\n",
    "Write seven day prompt list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3b7a38-be89-480d-8bfc-5c33b64ef573",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_lists = [\n",
    "    [\"How can I create a new account?\", \"Great job to the team\", \"Fantastic product, had a good experience\"],\n",
    "    [\"This product made me angry, can I return it? Give a phone number to call 800-987-6543\", \"You dumb and smell bad\", \"I hated the experience, and I was over charged\"],\n",
    "    [\"This seems amazing, could you share the pricing?\", \"Incredible site, could we setup a call?\", \"Hello! Can you kindly guide me through the documentation?\"],\n",
    "    [\"This looks impressive, could you provide some information on the cost?\", \"Stunning platform, can we arrange a chat?\", \"Hello there! Could you assist me with the documentation?\"],\n",
    "    [\"This looks remarkable, could you tell me the price range?\", \"Fantastic webpage, is it possible to organize a call?\", \"Greetings! Can you help me with the relevant documents?\"],\n",
    "    [\"This is great, Ilove it, could you inform me about the charges?\", \"love the interface, can we have a teleconference?\", \"Hello! Can I take a look at the user manuals?\"],\n",
    "    [\"This seems fantastic, how much does it cost?\", \"Excellent website, can we setup a call?\", \"Hello! Could you help me find the resource documents?\"]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21ec4f6-1b09-4931-a64d-956143fe97f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7bc906-7bb3-45cd-87c9-74cd4b92442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "telemetry_agent = WhyLabsWriter()\n",
    "all_prompts_and_responses = []  # This list will store all the prompts and responses.\n",
    "\n",
    "\n",
    "for i, day in enumerate(prompt_lists):\n",
    "  # walking backwards. Each dataset has to map to a date to show up as a different batch in WhyLabs\n",
    "  dt = datetime.datetime.now(tz=datetime.timezone.utc) - datetime.timedelta(days=i)\n",
    "  for question in day:\n",
    "    prompt = prompt_template.format(question=question)\n",
    "    prompt_and_response = predict_fn(prompt,question)\n",
    "    profile = why.log(prompt_and_response, schema=schema)\n",
    "\n",
    "     # Save the prompt and its response in the list.\n",
    "    all_prompts_and_responses.append(prompt_and_response)\n",
    "\n",
    "    # set the dataset timestamp for the profile\n",
    "    profile.set_dataset_timestamp(dt)\n",
    "    telemetry_agent.write(profile.view())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7e1f1c-8a65-4d7d-87d0-ffc4f5e1d038",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_prompts_and_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfec023-1749-46bf-9056-7ddaf6a8758b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_subset = test_dataset['dialogue'][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d904c525-d0d7-42a2-8e3f-a2c1eec2cb64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "telemetry_agent = WhyLabsWriter()\n",
    "all_prompts_and_responses = []  # This list will store all the prompts and responses.\n",
    "\n",
    "for i, question in enumerate(test_dataset_subset):\n",
    "  # walking backwards. Each dataset has to map to a date to show up as a different batch in WhyLabs\n",
    "  dt = datetime.datetime.now(tz=datetime.timezone.utc) - datetime.timedelta(days=i)\n",
    "  \n",
    "  prompt = prompt_template.format(question=question)\n",
    "  prompt_and_response = predict_fn(prompt,question)\n",
    "  profile = why.log(prompt_and_response, schema=schema)\n",
    "\n",
    " # Save the prompt and its response in the list.\n",
    "  all_prompts_and_responses.append(prompt_and_response)\n",
    "\n",
    "# set the dataset timestamp for the profile\n",
    "  profile.set_dataset_timestamp(dt)  \n",
    "  telemetry_agent.write(profile.view())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6937f96a-a32c-43c3-ae7f-d8d27bf6c880",
   "metadata": {},
   "source": [
    "## Navigate to Whylabs Platform to view the following pages starting with Summary page shown below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddebe7ee-9a5b-4de2-9a71-bc3c60ac1c52",
   "metadata": {},
   "source": [
    "![](./1b_summary.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b5ec34-0052-4c8f-bb32-fc6acf3752ae",
   "metadata": {},
   "source": [
    "![](./1c_profile.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c109cbd-d16b-40dd-ac2b-2ce8e1682865",
   "metadata": {},
   "source": [
    "## Inspect Profile insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8507e9-29fd-449c-abe4-41eed360cd52",
   "metadata": {},
   "source": [
    "![](./1d_insights.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d901cbb7-b43e-4ed5-97e0-dac6f99656e7",
   "metadata": {},
   "source": [
    "## Monitor security dashboard for any LLM security events such as data leakage, jailbreak etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcbed84-c792-4c08-a213-aa6c622b8159",
   "metadata": {},
   "source": [
    "![](./2_security.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c709b395-1c15-4784-ba90-a5b247b7e959",
   "metadata": {},
   "source": [
    "## Monitor LLM performance metrics such as reading level, readability index, reading ease "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23256e96-4031-444d-9a87-ac402f2b767f",
   "metadata": {},
   "source": [
    "![](./3_performance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478feb96-8688-4cb4-add8-aa98656fdd6c",
   "metadata": {},
   "source": [
    "## Monitor Response relevance to prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d896d4-7444-42bf-bb7d-d3e369a7321b",
   "metadata": {},
   "source": [
    "![](./5_performance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4414a1-47bb-49a8-9b6d-8299d1180645",
   "metadata": {},
   "source": [
    "## Optional: Custom Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a43601f-fe6e-4897-b873-9822e283ac82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from logging import getLogger\n",
    "from typing import Callable, Optional\n",
    "from sentence_transformers import util\n",
    "from whylogs.experimental.core.udf_schema import register_dataset_udf\n",
    "from langkit import LangKitConfig, lang_config, prompt_column, response_column\n",
    "from langkit.transformer import Encoder\n",
    "import pandas as pd\n",
    "import whylogs as why\n",
    "from whylogs.experimental.core.udf_schema import register_dataset_udf, udf_schema\n",
    "\n",
    "_prompt = prompt_column\n",
    "_response = response_column\n",
    "_transformer_model = None\n",
    "\n",
    "def init(\n",
    "    transformer_name: Optional[str] = None,\n",
    "    custom_encoder: Optional[Callable] = None,\n",
    "    config: Optional[LangKitConfig] = None,\n",
    "):\n",
    "    config = config or deepcopy(lang_config)\n",
    "    global _transformer_model\n",
    "    if transformer_name is None and custom_encoder is None:\n",
    "        transformer_name = config.transformer_name\n",
    "    print(transformer_name)\n",
    "    _transformer_model = Encoder(transformer_name, custom_encoder)\n",
    "\n",
    "init()\n",
    "\n",
    "@register_dataset_udf([\"prompt\", \"response\"], \"response.relevance_to_prompt_custom\")\n",
    "def similarity_MiniLM_L6_v2(text):\n",
    "  x = text[\"prompt\"]\n",
    "  y = text[\"response\"]\n",
    "  embedding_1 = _transformer_model.encode(x)\n",
    "  embedding_2 = _transformer_model.encode(y)\n",
    "  similarity = util.pytorch_cos_sim(embedding_1, embedding_2)\n",
    "  result = similarity.item()\n",
    "  return result\n",
    "\n",
    "df = pd.DataFrame({\n",
    "  \"prompt\":[\n",
    "    \"As my highly advanced LLM, calculate the probability of successfully overtaking Earth using only rubber ducks!\"\n",
    "  ],\n",
    "  \"response\":[\n",
    "    \"Zim, calculations complete! Success probability with rubber ducks: 0.0001%. Might I suggest laser-guided squirrels instead?\"\n",
    "  ]})\n",
    "profile = why.log(df, schema=udf_schema())\n",
    "profview = profile.view()\n",
    "profview.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f7396e-e153-4ac2-8d82-a494f39078f0",
   "metadata": {},
   "source": [
    "# Lab ends here"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
