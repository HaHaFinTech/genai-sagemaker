{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ef0c804-1b42-4b4d-958b-1dfb33c483f4",
   "metadata": {},
   "source": [
    "# Deploy a LLaMA 3.1 8B Instruct Model Adapter Using SageMaker Endpoints and SageMaker Large Model Inference (LMI) Container with the SageMaker Python SDK "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898a5386-536a-4977-9ffc-988519db65d3",
   "metadata": {},
   "source": [
    "In this example you will deploy a trained adapter of `LLaMA-3.1-8B-instruct`, to a SageMaker Managed Endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5203b9e8-e332-4629-881a-476c98a8eb44",
   "metadata": {},
   "source": [
    "Update the sagemaker SDK to the latest version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6796a3-cb84-44ad-863d-48fab445d543",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install sagemaker==2.229 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b969d8df-2897-4598-8a66-ed73f103059a",
   "metadata": {},
   "source": [
    "Baseline setup. Create clients for the boto3 SDK and default values for setting up the bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c7e0ab-084f-47fe-8638-bf3028d56a47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, boto3\n",
    "import sagemaker\n",
    "\n",
    "from sagemaker.model import Model\n",
    "from sagemaker import ModelPackage\n",
    "import time\n",
    "from datasets import load_dataset\n",
    "from pprint import pprint\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "sm_client = boto3.client('sagemaker')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6107bede-071b-433c-972d-ddbf9b00e47b",
   "metadata": {},
   "source": [
    "We will get the downloaded snapshot of base model from HuggingFace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e9c20a-57e1-49fa-87b3-7e70a3f8b794",
   "metadata": {},
   "source": [
    "## Large Model Inference (LMI) Containers\n",
    "\n",
    "In this example you will deploy your model using [SageMaker's Large Model Inference (LMI) Containers](https://docs.djl.ai/master/docs/serving/serving/docs/lmi/index.html).\n",
    "\n",
    "LMI containers are a set of high-performance Docker Containers purpose built for large language model (LLM) inference. With these containers, you can leverage high performance open-source inference libraries like vLLM, TensorRT-LLM, Transformers NeuronX to deploy LLMs on AWS SageMaker Endpoints. These containers bundle together a model server with open-source inference libraries to deliver an all-in-one LLM serving solution.\n",
    "\n",
    "The LMI container supports a variety of different backends, outlined in the table below. \n",
    "\n",
    "The model for this example can be deployed using the vLLM backend, which corresponds to the `djl-lmi` container image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8660ac53-d3db-4464-ac58-9440b5cc70f8",
   "metadata": {},
   "source": [
    "| Backend | SageMakerDLC | Example URI |\n",
    "| --- | --- | --- |\n",
    "|vLLM|djl-lmi|763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n",
    "|lmi-dist|djl-lmi|763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n",
    "|hf-accelerate|djl-lmi|763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n",
    "|tensorrt-llm|djl-tensorrtllm|763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.29.0-tensorrtllm0.11.0-cu124\n",
    "|transformers-neuronx|djl-neuronx|763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.29.0-neuronx-sdk2.19.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78440331-00b0-4516-9538-cdd155d41e21",
   "metadata": {},
   "source": [
    "In the following steps you will leverage the SageMaker Python SDK to build your model configuration and deploy it to SageMaker endpoint. There are alternative methods to do this as well, such as the Boto3 SDK, but the SM Python SDK reduces the amount of code necessary perform the same activities.\n",
    "\n",
    "The first step in model deployment is to [create a SageMaker Model object](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateModel.html). This consists of a unique name, a container image, and the environment configuration from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833d848a-8708-414b-be1e-676f43d05a88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store -r model_package_arn\n",
    "\n",
    "model_package_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b38a74-049b-46b0-9258-20179f18075d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "timestamp = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "endpoint_name=f\"endpopint-llama3-8b-instruct-adapter-{timestamp}\"\n",
    "\n",
    "ft_model = ModelPackage(\n",
    "    model_package_arn=model_package_arn,\n",
    "    sagemaker_session=sess,\n",
    "    role=role,\n",
    "    name=endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a15e14-dd62-4b7a-8e3d-2f5bc6bcff14",
   "metadata": {},
   "source": [
    "Now that you have a model object ready, you will use use the SageMaker Python SDK to create a SageMaker Managed Endpoint. The SDK eliminates some of the intermediate steps, such as creating an Endpoint Configuration.\n",
    "\n",
    "## Creating a new endpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dda03d-4d8d-4a48-b35d-b2799c2d7d1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(f\"Deploying model with endpoint name ep-{ft_model.name}\")\n",
    "ft_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g5.12xlarge\",\n",
    "    endpoint_name=f\"{ft_model.name}\",\n",
    "    container_startup_health_check_timeout=900,\n",
    "    wait=False\n",
    ")\n",
    "print(f\"\\nEndpoint deployed ===>\", ft_model.endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d2625f-b88d-4d06-a7de-710e61436a36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check the endpoint status\n",
    "while True:\n",
    "    response = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = response['EndpointStatus']\n",
    "    print(f\"Endpoint status: {status}\")\n",
    "    \n",
    "    if status == 'InService':\n",
    "        print(\"Endpoint is ready!\")\n",
    "        break\n",
    "    elif status == 'Failed':\n",
    "        print(\"Endpoint creation failed.\")\n",
    "        break\n",
    "    \n",
    "    # Wait for 30 seconds before checking again\n",
    "    time.sleep(30)\n",
    "\n",
    "print(f\"Final endpoint status: {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0060d16e-5135-48de-830a-2700b9701b6b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pick a random prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b1168f-c3b0-4ec8-b66c-707833592b28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_summarization_prompts(data_point):\n",
    "    full_prompt =f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "                    You are an AI assistant trained to summarize conversations. Provide a concise summary of the dialogue, capturing the key points and overall context.\n",
    "                    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "                    Summarize the following conversation:\n",
    "\n",
    "                    {data_point[\"dialogue\"]}\n",
    "                    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "                    Here's a concise summary of the conversation in a single sentence:\n",
    "\n",
    "                    <|eot_id|>\"\"\"\n",
    "    return {\"prompt\": full_prompt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d579721-50a4-4156-b663-0a60e1227280",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HF dataset that we will be working with \n",
    "dataset_name=\"Samsung/samsum\"\n",
    "    \n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(dataset_name, split=\"test\")\n",
    "\n",
    "random_row = dataset.shuffle().select(range(1))[0]\n",
    "random_row\n",
    "# Add system message to each conversation\n",
    "#columns_to_remove = list(random_row.features)\n",
    "\n",
    "random_prompt=create_summarization_prompts(random_row)\n",
    "pprint(random_prompt)\n",
    "#columns_to_remove\n",
    "#dataset = random_row.map(create_summarization_prompts, remove_columns=columns_to_remove,batched=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daf1e3c-7338-4ff7-8a8b-8bccaf8711b2",
   "metadata": {},
   "source": [
    "# Run Inference\n",
    "\n",
    "With your endpoint successfully deployed, you will want to test it to ensure that it is fully functional.\n",
    "\n",
    "To do so, you will take a piece of sample text and summarize it using your deployed model. This sample text was pulled from the [ECTSum dataset](https://huggingface.co/datasets/mrSoul7766/ECTSum)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28c4f02-5ed9-464c-8e42-bf2fc06e0656",
   "metadata": {},
   "source": [
    "Using the sample article and prompt template, invoke the model to view the structure of the response and its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6256f9f-6885-4091-9c61-4eacb536488c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "\n",
    "#endpoint_name = \"deploy-llama3-8b-instruct-adapter-v2\"\n",
    "print(endpoint_name)\n",
    "\n",
    "llm = sagemaker.Predictor(\n",
    "    endpoint_name = endpoint_name,\n",
    "    sagemaker_session = sess,\n",
    "    serializer = sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer = sagemaker.deserializers.JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155ff092-a144-40d1-8d2a-a41d7776409b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = llm.predict(\n",
    "    {\n",
    "        \"inputs\": random_prompt['prompt'],\n",
    "        \"parameters\": {\n",
    "            \"do_sample\":True,\n",
    "            \"max_new_tokens\":256,\n",
    "            \"top_p\":0.9,\n",
    "            \"temperature\":0.6,\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "response['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3a0648-ee84-4e50-ad67-bac9677c5cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
