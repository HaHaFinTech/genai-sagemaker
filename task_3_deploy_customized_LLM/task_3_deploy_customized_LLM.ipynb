{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "993dcf59-dd85-4a31-a8cc-02c5d4be5dfe",
   "metadata": {},
   "source": [
    "# Deploy a LLaMA-3.1-8B-Instruct Model and Quantized version of the same model using SageMaker Endpoints and SageMaker Large Model Inference (LMI) Container with the SageMaker Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c01a4b2-b0d3-420c-8335-4efd984bb5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sagemaker==2.229 --upgrade --quiet --no-warn-conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d192f870-74ee-4470-baf5-93729291a6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import boto3, sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0441ee-5202-42be-a4b1-68327f68ff71",
   "metadata": {},
   "source": [
    "Baseline SageMaker setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9183bf15-3f3f-4d70-91da-fe269ff421b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker version: 2.229.0\n"
     ]
    }
   ],
   "source": [
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "\n",
    "sm_client = boto3.client('sagemaker')\n",
    "\n",
    "print(f\"SageMaker version: {sagemaker.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805fd112-9ddc-4335-963f-c98371aa96f9",
   "metadata": {},
   "source": [
    "## Large Model Inference (LMI) Containers\n",
    "\n",
    "In this example you will deploy your model using [SageMaker's Large Model Inference (LMI) Containers](https://docs.djl.ai/master/docs/serving/serving/docs/lmi/index.html).\n",
    "\n",
    "LMI containers are a set of high-performance Docker Containers purpose built for large language model (LLM) inference. With these containers, you can leverage high performance open-source inference libraries like vLLM, TensorRT-LLM, Transformers NeuronX to deploy LLMs on AWS SageMaker Endpoints. These containers bundle together a model server with open-source inference libraries to deliver an all-in-one LLM serving solution.\n",
    "\n",
    "The LMI container supports a variety of different backends, outlined in the table below. \n",
    "\n",
    "The model for this example can be deployed using the LMI-Dist| Backend | SageMakerDLC | Example URI |\n",
    "| --- | --- | --- |\n",
    "|vLLM|djl-lmi|763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n",
    "|lmi-dist|djl-lmi|763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n",
    "|hf-accelerate|djl-lmi|763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n",
    "|tensorrt-llm|djl-tensorrtllm|763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.29.0-tensorrtllm0.11.0-cu124\n",
    "|transformers-neuronx|djl-neuronx|763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.29.0-neuronx-sdk2.19.1 backend, which corresponds to the `djl-lmi` container image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca2fcc0-5f8b-4377-9624-58bed45052be",
   "metadata": {},
   "source": [
    "| Backend | SageMakerDLC | Example URI |\n",
    "| --- | --- | --- |\n",
    "|vLLM|djl-lmi|763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n",
    "|lmi-dist|djl-lmi|763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n",
    "|hf-accelerate|djl-lmi|763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n",
    "|tensorrt-llm|djl-tensorrtllm|763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.29.0-tensorrtllm0.11.0-cu124\n",
    "|transformers-neuronx|djl-neuronx|763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.29.0-neuronx-sdk2.19.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f91002f-04c1-450f-a414-ea262870a1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "LMI_VERSION = \"0.29.0\"\n",
    "LMI_FRAMEWORK = 'djl-lmi'\n",
    "\n",
    "lmidist_image = sagemaker.image_uris.retrieve(framework=LMI_FRAMEWORK, region=region, version=LMI_VERSION)\n",
    "\n",
    "print(f\"Inference Image: {lmidist_image}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dd8c2e-8086-4919-a46b-f6dd61dc358f",
   "metadata": {},
   "source": [
    "Next you will need to specify configuration of the LMI container to allow the model artifact to be downloaded, and provide optimized parameters to allow the model to run on the chosen instance size/type.\n",
    "\n",
    "There are 2 methods to supply configuration to the LMI container:\n",
    "1. Create a `serving.properties` file and include it inside the compressed model artifact. This has the benefit of ensuring that no configuration information needs to be shared, as long as you have the model artifact. However it creates rigidity as it is tightly coupled and creates complexity when deploying on different instance types.\n",
    "2. Provide a set of Environment Variables to the SageMaker Model object. This provides flexibility by storing the LMI configuration information inside the SageMaker Model configuration step.\n",
    "\n",
    "In this example, you will leverage Environment Variables to configure the LMI container.\n",
    "\n",
    "For deploying HuggingFace models, the `HF_MODEL_ID` parameter is dual purpose and can be either the HuggingFace Model ID, or an S3 location of the model artifacts. If you specify the Model ID, the artifacts will be downloaded when the endpoint is created.\n",
    "\n",
    "Specific optimizations for this smaller instance size are limiting the `max_model_len` parameter to 20,000 (down from LLaMA-3.1-8B's 128k default) and reducing the `gpu_memory_utilization` to 0.5 to help prevent CUDA OOM errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc0fda0-49cd-4c31-8787-bb352977f62d",
   "metadata": {},
   "source": [
    "### Model License Information\n",
    "Meta Llama-3.1 model a gated model. To use this model you have to agree to the license agreement and request access before the model can be used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe72cc48-abcd-4d74-b659-18d33e051535",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Base model location (needs to be replaced by API call to model registry)\n",
    "base_model_location = \"s3://YOU_S3_URI\"\n",
    "\n",
    "#\n",
    "# Adapter(s) location (needs to be replaced by API call to model registry with optional uncompress)\n",
    "adapter_model_location = \"s3://YOU_S3_URI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d0f5e4-c352-4007-901d-300d3c26fc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf lora-multi-adapter\n",
    "!mkdir -p lora-multi-adapter/adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8bd0ac-8022-4220-a85d-923d17b79d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lora-multi-adapter/serving.properties\n",
    "option.model_id={base_model_location}\n",
    "option.max_rolling_batch_size=16\n",
    "option.rolling_batch=lmi-dist\n",
    "option.max_rolling_batch_prefill_tokes=4096\n",
    "option.max_model_len=4096\n",
    "option.enable_lora=true\n",
    "option.gpu_memory_utilization=0.8\n",
    "option.max_lora_rank=64\n",
    "option.max_cpu_loras=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5f3d7e-57ac-41f1-8e5b-b088ee634a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_code_prefix = \"workshop/multi-lora/Llama-2-7b-fp16\"\n",
    "s3_adapters_location = sess.upload_data(\"lora-multi-adapter\", bucket, s3_code_prefix)\n",
    "print(s3_adapters_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741bc715-c189-4f06-a539-63e642d35d3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d19701e-83c0-40e0-bac9-865db6a08d4e",
   "metadata": {},
   "source": [
    "Helper function to test latency of the deployed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1c43cc-8758-43f0-8e5f-14a2624b7d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def run_perf_test(llm, num_iterations, prompt):\n",
    "    results = []\n",
    "    for i in range(0, num_iterations):\n",
    "        start = time.time()\n",
    "        res = llm.predict({\"inputs\": prompt})\n",
    "        results.append((time.time() - start) * 1000)\n",
    "    \n",
    "    print(\"\\nPrediction latency: \\n\")\n",
    "    print(\"P95: \" + str(np.percentile(results, 95)) + \" ms\")\n",
    "    print(\"P90: \" + str(np.percentile(results, 90)) + \" ms\")\n",
    "    print(\"Average: \" + str(np.average(results)) + \" ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1634e71f-a190-408f-bcb9-a314cd057e30",
   "metadata": {},
   "source": [
    "## LMI-Dist (recommended framework for FM deployments on Amazon SageMaker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0c37f0-6a2e-4a0c-8630-70429dacaabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = \"ml.g5.2xlarge\"\n",
    "container = lmidist_image\n",
    "model_name = sagemaker.utils.name_from_base(\"llama31-8b-lmidist\")\n",
    "endpoint_name = model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136ba07b-057b-4d29-8521-6775cdcc6575",
   "metadata": {},
   "source": [
    "In the following steps you will leverage the SageMaker Python SDK to build your model configuration and deploy it to SageMaker endpoint. There are alternative methods to do this as well, such as the Boto3 SDK, but the SM Python SDK reduces the amount of code necessary perform the same activities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8b0254-c0d1-428c-a186-86e95c9e7173",
   "metadata": {},
   "source": [
    "We need to update our model package with correct inference parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0045ba7-7660-4ed3-a08b-d796f424bd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#response = sm_client.update_model_package(\n",
    "#    ModelPackageArn=latest_model_package_arn,\n",
    "#    InferenceSpecification={\n",
    "#        'Containers': [\n",
    "#            {\n",
    "#                'Image': inference_image_uri,\n",
    "#                'ModelDataUrl': model_data_uri\n",
    "#            },\n",
    "#        ],\n",
    "#        'Environment': lmidist_config,\n",
    "#        'SupportedTransformInstanceTypes': ['ml.g5.12xlarge', 'ml.p3.8xlarge'],\n",
    "#        'SupportedRealtimeInferenceInstanceTypes': ['ml.g5.2xlarge'],\n",
    "#        'SupportedContentTypes': ['application/json'],\n",
    "#        'SupportedResponseMIMETypes': ['application/json']\n",
    "#    }\n",
    "#)\n",
    "#model_package_arn = response[\"ModelPackageArn\"]\n",
    "#print(f\"update registered model's inference spec: {model_package_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b691117-7ba4-45d0-bae7-d5ce83a21b9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12049273-2d20-45e6-9d56-1afc3d33bcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "lmi_model = sagemaker.Model(\n",
    "    image_uri = lmidist_image,\n",
    "    model_data = {\n",
    "        'S3DataSource': {\n",
    "            'S3Uri': adapter_model_location + \"/\",\n",
    "            'S3DataType': \"S3Prefix\",\n",
    "            'CompressionType': \"None\"\n",
    "        }\n",
    "    },\n",
    "    role = role,\n",
    "    name = model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96aca52-400d-4813-ab3b-96ff4571fc45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8073145d-d507-4cc8-8ab8-5ab595ac2332",
   "metadata": {},
   "source": [
    "Now that you have a model object ready, you will use use the SageMaker Python SDK to create a SageMaker Managed Endpoint. The SDK eliminates some of the intermediate steps, such as creating an Endpoint Configuration.\n",
    "\n",
    "***Note: creating a new endpoint can take between 8-15 minutes.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dea071-a2df-4dc2-8980-2c9b8ec605cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lmi_model.deploy(\n",
    "    initial_instance_count = 1,\n",
    "    instance_type = instance_type,\n",
    "    container_startup_health_check_timeout = 900,\n",
    "    endpoint_name = endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152b5bf7-54b2-4d23-8a4f-728f41a6c1c7",
   "metadata": {},
   "source": [
    "With your endpoint successfully deployed, you will want to test it to ensure that it is fully functional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c6a984-289d-44f2-9a42-5ecbf35744ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = sagemaker.Predictor(\n",
    "    endpoint_name = endpoint_name,\n",
    "    sagemaker_session = sess,\n",
    "    serializer = sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer = sagemaker.deserializers.JSONDeserializer(),\n",
    ")\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": \"¿Qué es Amazon SageMaker?\",\n",
    "    \"adapters\": [\"es\"],\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 128,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.6,\n",
    "    },\n",
    "}\n",
    "\n",
    "res = llm.predict(payload)\n",
    "print(\"\\n---\\n\",res[\"generated_text\"], \"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e467d235-9231-4fdb-9605-85697d692561",
   "metadata": {},
   "source": [
    "Let's get base model latency numbers (we will compare them to latency of quantized model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2d6a33-d531-4f45-87c7-2be370f14662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Calculate runtime performance\n",
    "# \n",
    "run_perf_test(llm = llm, num_iterations = 10, prompt = prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055faf5b-2b41-47ed-aeda-c00d266fee3d",
   "metadata": {},
   "source": [
    "**Do NOT forget to delete unused endpoint to avoid unnessary charges to your account**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654b369f-8f8d-4039-baed-9f2206fdd32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.delete_model()\n",
    "llm.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8585a7-6264-4fc7-9151-e9feb3d20250",
   "metadata": {},
   "source": [
    "## (Optional) Deploy quantized (GPTQ) Llama-3.1-8B using LMI container with lmi-dist framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5934e2-c85d-41fe-9be7-448ee0fa9a76",
   "metadata": {},
   "source": [
    "### In the next few cells we deploy a quantized version of the same model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14d7b43-e9cc-440e-b02d-e29325aaf142",
   "metadata": {},
   "outputs": [],
   "source": [
    "lmidist_quantized_config = {\n",
    "    \"HF_MODEL_ID\": \"neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16\",\n",
    "    #\"HF_MODEL_ID\": \"YOUR_S3URI\",\n",
    "    \"OPTION_ROLLING_BATCH\": \"lmi-dist\",\n",
    "    \"OPTION_TENSOR_PARALLEL_DEGREE\": \"max\",\n",
    "    \"OPTION_OUTPUT_FORMATTER\": \"json\",\n",
    "    \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"64\",\n",
    "    \"OPTION_MAX_ROLLING_BATCH_PREFILL_TOKENS\": \"8192\",\n",
    "    \"OPTION_MAX_MODEL_LEN\": \"8192\",\n",
    "    \"OPTION_GPU_MEMORY_UTILIZATION\": \"0.8\",\n",
    "    \"OPTION_QUANTIZE\": \"gptq\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481abeb5-380e-492f-ad0d-90123725c3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = \"ml.g5.2xlarge\"\n",
    "container = lmidist_image\n",
    "config = lmidist_quantized_config\n",
    "model_name = sagemaker.utils.name_from_base(\"llama31-8b-q-lmidist\")\n",
    "endpoint_name = model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ccecda-9faf-41d2-8fc5-fadd934982b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lmi_model = sagemaker.Model(\n",
    "    image_uri = container,\n",
    "    env = config,\n",
    "    role = role,\n",
    "    name = model_name\n",
    ")\n",
    "\n",
    "lmi_model.deploy(\n",
    "    initial_instance_count = 1,\n",
    "    instance_type = instance_type,\n",
    "    container_startup_health_check_timeout = 900,\n",
    "    endpoint_name = endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab0af51-5d50-48f1-9956-e1b0dd7b100f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = sagemaker.Predictor(\n",
    "    endpoint_name = endpoint_name,\n",
    "    sagemaker_session = sess,\n",
    "    serializer = sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer = sagemaker.deserializers.JSONDeserializer(),\n",
    ")\n",
    "\n",
    "prompt = \"What is Amazon SageMaker?\"\n",
    "\n",
    "res = llm.predict({\"inputs\": prompt})\n",
    "print(\"\\n---\\n\",res[\"generated_text\"], \"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afb3277-eedf-4067-a003-dcb30261874e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Calculate runtime performance\n",
    "# \n",
    "run_perf_test(llm = llm, num_iterations = 10, prompt = prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5cc7d8-b3c7-4505-b268-783913ce02c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.delete_model()\n",
    "llm.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bbb8bb-6099-4e75-8dd9-d96ef9b64765",
   "metadata": {},
   "source": [
    "## (Optional) Deploy Llama-3.1-8B using TensorRT-LLM backend with Just-In-Time (JIT) compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dc585e-1360-475d-9d01-d70c7bd8c49b",
   "metadata": {},
   "source": [
    "**Please note that deployment using TensorRT-LLM backend with JIT requires bigger instance (g5.8xlarge as a minimum) because of additional memeory requirements for compilation process. You can still deploy pre-compiled Llama-3.1-8B on g5.2xlarge**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd3c5cf-a233-422c-9c8a-56e5be29c7c6",
   "metadata": {},
   "source": [
    "**This option activity provided for education purpose and it can be used when running the notebook in AWS account that have access to required instance types (like your personal/organization account)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0f51d2-dcf6-42b1-ad98-543b1d36a120",
   "metadata": {},
   "source": [
    "***!!! Do NOT run this during workshop if you don't have access to at least g5.8xlarge !!!***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1030653-0843-491f-9b35-b23780097a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "LMI_VERSION = \"0.29.0\"\n",
    "LMI_FRAMEWORK = 'djl-tensorrtllm'\n",
    "\n",
    "tensorrtllm_image = sagemaker.image_uris.retrieve(framework=LMI_FRAMEWORK, region=region, version=LMI_VERSION)\n",
    "\n",
    "print(f\"Inference Image: {tensorrtllm_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d56a128-1d6e-4323-a4b4-df86c3fecddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# TensorRT-LLM 0.11.0 (included in LMI 0.29.0) does NOT support Llama-3.1.\n",
    "# We will use Llama-3 instead\n",
    "#\n",
    "tensorrtllm_config = {\n",
    "    \"HF_MODEL_ID\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    \"HF_TOKEN\": \"YOUR_HF_TOKEN\",\n",
    "    #\"HF_MODEL_ID\": \"YOUR_S3_URI\",\n",
    "    \"OPTION_ROLLING_BATCH\": \"trtllm\",\n",
    "    \"OPTION_TENSOR_PARALLEL_DEGREE\": \"max\",\n",
    "    \"OPTION_MAX_NUM_TOKENS\": \"4096\",\n",
    "    \"OPTION_GPU_MEMORY_UTILIZATION\": \"0.8\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dd0520-4e9d-48ae-95fa-6d7cee71b2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = \"ml.g5.8xlarge\"\n",
    "container = tensorrtllm_image\n",
    "config = tensorrtllm_config\n",
    "model_name = sagemaker.utils.name_from_base(\"llama31-8b-trtllm\")\n",
    "endpoint_name = model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a09a79-1c0a-4a18-8fa6-859e33b785fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "lmi_model = sagemaker.Model(\n",
    "    image_uri = container,\n",
    "    env = config,\n",
    "    role = role,\n",
    "    name = model_name\n",
    ")\n",
    "\n",
    "lmi_model.deploy(\n",
    "    initial_instance_count = 1,\n",
    "    instance_type = instance_type,\n",
    "    container_startup_health_check_timeout = 900,\n",
    "    endpoint_name = endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd01233-5ccc-4194-8d03-b59ece243edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = sagemaker.Predictor(\n",
    "    endpoint_name = endpoint_name,\n",
    "    sagemaker_session = sess,\n",
    "    serializer = sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer = sagemaker.deserializers.JSONDeserializer(),\n",
    ")\n",
    "\n",
    "prompt = \"What is Amazon SageMaker?\"\n",
    "\n",
    "res = llm.predict({\"inputs\": prompt})\n",
    "print(\"\\n---\\n\",res[\"generated_text\"], \"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6513976d-077e-477e-b367-4c324a0d65be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Calculate runtime performance\n",
    "# \n",
    "run_perf_test(llm = llm, num_iterations = 10, prompt = prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf9ffeb-04fc-4b87-89ea-94adc2894816",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.delete_model()\n",
    "llm.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd94f5e-280a-4e13-8f0d-29f3d112480e",
   "metadata": {},
   "source": [
    "## (Optional) Deploy Llama-3.1-8B on Inferentia2 with Just-In-Time (JIT) compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f939e01-2744-4320-8dc7-271d6318aa40",
   "metadata": {},
   "source": [
    "**Please note that deployment on Inferentia2 with JIT requires bigger instance (inf2.8xlarge as a minimum) because of additional memeory requirements for compilation process.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92da3c4f-e194-4fb2-b397-d861521eef97",
   "metadata": {},
   "source": [
    "**This option activity provided for education purpose and it can be used when running the notebook in AWS account that have access to required instance types (like your personal/organization account)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1928326-c741-4ee4-8bcd-095bda90155c",
   "metadata": {},
   "source": [
    "***!!! Do NOT run this during workshop if you don't have access to at least inf2.8xlarge !!!***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f41251-d313-4c40-b01e-d7a54d62286c",
   "metadata": {},
   "outputs": [],
   "source": [
    "LMI_VERSION = \"0.29.0\"\n",
    "LMI_FRAMEWORK = 'djl-neuronx'\n",
    "\n",
    "neuronx_image = sagemaker.image_uris.retrieve(framework=LMI_FRAMEWORK, region=region, version=LMI_VERSION)\n",
    "\n",
    "print(f\"Inference Image: {neuronx_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d34a24e-140f-45ce-9371-237325181a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "neuronx_config = {\n",
    "    \"HF_MODEL_ID\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    \"HF_TOKEN\": \"YOUR_HF_TOKEN\",\n",
    "    #\"HF_MODEL_ID\": \"YOUR_S3_URI\",\n",
    "    \"OPTION_ROLLING_BATCH\": \"auto\",\n",
    "    \"OPTION_TENSOR_PARALLEL_DEGREE\": \"2\",\n",
    "    \"OPTION_N_POSITIONS\": \"4096\",\n",
    "    \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"8\",\n",
    "    \"OPTION_DTYPE\": \"fp16\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade9a267-5cf8-4744-92eb-ea40e9065107",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = \"ml.inf2.8xlarge\"\n",
    "container = neuronx_image\n",
    "config = neuronx_config\n",
    "model_name = sagemaker.utils.name_from_base(\"llama31-8b-neuronx\")\n",
    "endpoint_name = model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48c0143-7738-4f2a-ad3b-19cbd072019c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lmi_model = sagemaker.Model(\n",
    "    image_uri = container,\n",
    "    env = config,\n",
    "    role = role,\n",
    "    name = model_name\n",
    ")\n",
    "\n",
    "lmi_model.deploy(\n",
    "    initial_instance_count = 1,\n",
    "    instance_type = instance_type,\n",
    "    container_startup_health_check_timeout = 900,\n",
    "    endpoint_name = endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4574f8-02fa-477c-a2bd-2d127e52d399",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = sagemaker.Predictor(\n",
    "    endpoint_name = endpoint_name,\n",
    "    sagemaker_session = sess,\n",
    "    serializer = sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer = sagemaker.deserializers.JSONDeserializer(),\n",
    ")\n",
    "\n",
    "prompt = \"What is Amazon SageMaker?\"\n",
    "\n",
    "res = llm.predict({\"inputs\": prompt})\n",
    "print(\"\\n---\\n\",res[\"generated_text\"], \"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c623dbcc-5482-4187-801c-a23f6e67d7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Calculate runtime performance\n",
    "# \n",
    "run_perf_test(llm = llm, num_iterations = 10, prompt = prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39446ba1-3075-4af6-8971-422bc93a4100",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.delete_model()\n",
    "llm.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56776ee-e79d-4127-a503-02433d138677",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
