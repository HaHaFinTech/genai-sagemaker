{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8dae933-0b5f-4c6f-b9fc-145e467071b9",
   "metadata": {},
   "source": [
    "## Comparing Model Performance of Summarization Accuracy After Fine-Tuning\n",
    "\n",
    "In this example, we will take the pre-existing SageMaker endpoints that you deployed in previous exercises and use them to generate data that can be leveraged for quality comparison. This data can be used to take a quantitative approach towards judge the efficacy of fine-tuning your models.\n",
    "\n",
    "This example will run through samples of the [Samsum dataset](https://huggingface.co/datasets/Samsung/samsum) (paper [here](https://aclanthology.org/D19-5409/)) on the HuggingFace data hub to generate summaries of earnings calls transcripts and use the [fmeval library](https://github.com/aws/fmeval) for analysis on those summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa600089-3179-4443-8227-48b9b3f56f16",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aaaf7d-da6a-40d5-81ce-582a84f7f5a0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install the fmeval package\n",
    "!pip install -U datasets==2.21.0\n",
    "!pip install -U jsonlines==4.0.0\n",
    "!pip install -U fmeval==1.2.0\n",
    "!pip install -U py7zr==0.22.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594b214a-aedc-4b4c-bf4c-37fd048c2b58",
   "metadata": {},
   "source": [
    "Here you will use the HuggingFace datasets package to load the Samsum dataset. The dataset is pre-split into training and test data, so you can simply take that split using the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f920ab-e925-4b96-9cbc-50b36b6af3fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "test_dataset  = load_dataset(\"Samsung/samsum\", split=\"test\")\n",
    "\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e28999-5260-4b99-94e7-37ac6d2e6161",
   "metadata": {},
   "source": [
    "You can see the test dataset has 819 items in it, and they can be accessed via index. The items include the transcription of the earnings call and a short summary of that dialogue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534a28c3-889c-4375-9b89-544fee91093d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset[204]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dd0207-0d93-490c-bc07-21d96afd31d9",
   "metadata": {},
   "source": [
    "Create the client objects for calling SageMaker APIs, and supply the names of the SageMaker endpoints you created for the base and fine-tuned versions of the model. If you did not deploy both models, you can simply set them to the same endpoint name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc561a07-56ab-40ed-ac88-faa76c73eef7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "boto_session = boto3.session.Session()\n",
    "region = boto_session.region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81acb9f-4bc6-4b54-9c42-c04fcc7b31c6",
   "metadata": {},
   "source": [
    "## Validate endpoint functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8a8297-ce56-4793-8aca-095382e76175",
   "metadata": {},
   "source": [
    "### Reference your base and fine-tuned endpoints\n",
    "\n",
    "# ***\n",
    "# NOTE: PROVIDE YOUR UNIQUE ENDPOINTS HERE OR YOU WILL GET ERRORS\n",
    "# ***\n",
    "\n",
    "__If you will be evaluating a model with swappable LoRA adapters, you can use the same endpoint name for both base and tuned with varying adapter references in your inference payload.__\n",
    "\n",
    "Omitting the adapter will result in the base model being used without any adapter, and specifying an adapter array with it's name will use that adapter for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdef9927-14cb-4b31-bef8-12c8509dba6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENTER YOUR ENDPOINTS HERE\n",
    "base_endpoint_name = \"<YOUR_MODEL_ENDPOINT_HERE>\"\n",
    "tuned_endpoint_name = \"<YOUR_MODEL_ENDPOINT_HERE>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd23fe0-49d0-4b10-aa07-90309913a1b0",
   "metadata": {},
   "source": [
    "### As a quick test, you will take a base prompt and sample from the dataset to verify that the endpoints provided will work for the upcoming test runs. \n",
    "\n",
    "You can also use this as a subjective comparison of the 3 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b74741-ac5e-40f2-8e96-eb8fe9022322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "prompt = f\"\"\"\n",
    "<|begin_of_text|>\n",
    "<|start_header_id|>system<|end_header_id|>\n",
    "You are a helpful assistant who is an expert in summarizing conversations.\n",
    "<|eot_id|>\n",
    "\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "Summarize the provided conversation in 2 sentences.\n",
    "\n",
    "{test_dataset[0]['dialogue']}\n",
    "\n",
    "Provide the summary directly, without any introduction or preamble. Do not start the response with \"Here is a...\".<|eot_id|>\n",
    "\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "base_payload = {\"inputs\": prompt,\"parameters\": {\"do_sample\": True,\"top_p\": 0.9,\"temperature\": 0.8,\"max_new_tokens\": 256,},}\n",
    "tuned_payload = {\"inputs\": prompt,\"parameters\": {\"do_sample\": True,\"top_p\": 0.9,\"temperature\": 0.8,\"max_new_tokens\": 256,}, \"adapters\":[\"sum\"]}\n",
    "tuned5_payload = {\"inputs\": prompt,\"parameters\": {\"do_sample\": True,\"top_p\": 0.9,\"temperature\": 0.8,\"max_new_tokens\": 256,}, \"adapters\":[\"sum5\"]}\n",
    "\n",
    "\n",
    "base_predictor = sagemaker.Predictor(\n",
    "    endpoint_name = base_endpoint_name,\n",
    "    sagemaker_session = sess,\n",
    "    serializer = sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer = sagemaker.deserializers.JSONDeserializer(),\n",
    ")\n",
    "\n",
    "base_predictor_response = base_predictor.predict(base_payload)\n",
    "\n",
    "print(f\"Base Model:\\n{base_predictor_response['generated_text']}\")\n",
    "print(\"\\n ================ \\n\")\n",
    "\n",
    "tuned_predictor = sagemaker.Predictor(\n",
    "    endpoint_name = tuned_endpoint_name,\n",
    "    sagemaker_session = sess,\n",
    "    serializer = sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer = sagemaker.deserializers.JSONDeserializer(),\n",
    ")\n",
    "\n",
    "tuned_predictor_response = tuned_predictor.predict(tuned_payload)\n",
    "\n",
    "\n",
    "print(f\"Fine-Tuned Model (1 Epoch):\\n{tuned_predictor_response['generated_text']}\")\n",
    "print(\"\\n ================ \\n\")\n",
    "\n",
    "tuned5_predictor = sagemaker.Predictor(\n",
    "    endpoint_name = tuned_endpoint_name,\n",
    "    sagemaker_session = sess,\n",
    "    serializer = sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer = sagemaker.deserializers.JSONDeserializer(),\n",
    ")\n",
    "\n",
    "tuned5_predictor_response = tuned_predictor.predict(tuned5_payload)\n",
    "\n",
    "\n",
    "print(f\"Fine-Tuned Model (5 Epochs):\\n{tuned5_predictor_response['generated_text']}\")\n",
    "print(\"\\n ================ \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9228060-c528-4fc2-adf4-df8e7fe51b83",
   "metadata": {},
   "source": [
    "### Building the test dataset files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eccc4e8-f393-4e95-8c03-d02321520654",
   "metadata": {},
   "source": [
    "fmeval requires the test data to be in a flat file format, so this section will take the examples from the ECTSum dataset object and store them on the local filesystem in jsonlines format.\n",
    "\n",
    "This file will include the source transcript and the summary so model outputs can be evaluated against ground truth data.\n",
    "\n",
    "The code uses 10 samples as a base to show functionality, but you can increase that number to gather more datapoints. The more samples, the longer the evaluation will take. If you are running this in a live workshop, it is advised to not go beyond 50 for time purposes. (50 samples will take around 5 minutes to generate the analysis per model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5803e7e-4803-4e95-9059-da361b08b3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "\n",
    "#Change this to whatever number of samples you'd like to run your analysis on. Set to \"max\" to use the whole set if you dont want to set a number.\n",
    "#number_of_samples_to_take=\"max\"\n",
    "number_of_samples_to_take=10\n",
    "\n",
    "if number_of_samples_to_take == \"max\" or len(test_dataset) < number_of_samples_to_take:\n",
    "    number_of_samples_to_take = len(test_dataset)\n",
    "    \n",
    "output_file = \"samsum_summary_sample.jsonl\"\n",
    "\n",
    "# For each line in `input_file`, invoke the model using the input from that line,\n",
    "# augment the line with the invocation results, and write the augmented line to `output_file`.\n",
    "\n",
    "with jsonlines.open(output_file, \"w\") as output_fh:\n",
    "    for i in range(number_of_samples_to_take):\n",
    "        sample = test_dataset[i]\n",
    "        line = {}\n",
    "        text = sample[\"dialogue\"]\n",
    "        line[\"dialogue\"] = text\n",
    "        summary = sample[\"summary\"]\n",
    "        line[\"summary\"] = summary\n",
    "        print(f'\\rCompleted Row {i+1} of {number_of_samples_to_take}', end=\"\")        \n",
    "        output_fh.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6788d618-db06-42a6-ac13-9785f5269788",
   "metadata": {},
   "source": [
    "### fmeval Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44478b6a-6e94-43b0-bdcc-3bf3e0713654",
   "metadata": {},
   "source": [
    "Now that the source data is prepared, you are ready to configure fmeval to run your tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3d9e90-81af-403e-83f0-28f6a5ee4a10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fmeval.data_loaders.data_config import DataConfig\n",
    "from fmeval.constants import MIME_TYPE_JSONLINES\n",
    "from fmeval.eval_algorithms.summarization_accuracy import SummarizationAccuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae741ef1-85f2-4e0f-afc9-017316ba401b",
   "metadata": {},
   "source": [
    "#### Data Config Setup\n",
    "\n",
    "Below, we create a DataConfig for the local dataset file we just created, `ectsum_summary_sample.jsonl`.\n",
    "\n",
    "- `dataset_name` is just an identifier for your own reference\n",
    "- `dataset_uri` is either a local path to a file or an S3 URI\n",
    "- `dataset_mime_type` is the MIME type of the dataset. Currently, JSON and JSON Lines are supported.\n",
    "- `model_input_location`, `target_output_location`, and `model_output_location` are JMESPath queries used to find the model inputs, target outputs, and model outputs within the dataset. The values that you specify here depend on the structure of the dataset itself. Take a look at trex_sample_with_model_outputs.jsonl to see where \"question\", \"answers\", and \"model_output\" show up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1cf5c4-ae36-4ac7-8226-549792882dfa",
   "metadata": {},
   "source": [
    "Because fmeval creates its reports based on the dataset name, we will need to create 3 objects to map to both the different variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8486b7-1a44-40a3-9657-984015688942",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_data_config = DataConfig(\n",
    "    dataset_name=\"base_model_samsum_sample\",\n",
    "    dataset_uri=\"samsum_summary_sample.jsonl\",\n",
    "    dataset_mime_type=MIME_TYPE_JSONLINES,\n",
    "    model_input_location=\"dialogue\",\n",
    "    target_output_location=\"summary\",\n",
    ")\n",
    "\n",
    "tuned_model_data_config = DataConfig(\n",
    "    dataset_name=\"tuned_model_samsum_sample\",\n",
    "    dataset_uri=\"samsum_summary_sample.jsonl\",\n",
    "    dataset_mime_type=MIME_TYPE_JSONLINES,\n",
    "    model_input_location=\"dialogue\",\n",
    "    target_output_location=\"summary\",\n",
    ")\n",
    "\n",
    "tuned5_model_data_config = DataConfig(\n",
    "    dataset_name=\"tuned5_model_samsum_sample\",\n",
    "    dataset_uri=\"samsum_summary_sample.jsonl\",\n",
    "    dataset_mime_type=MIME_TYPE_JSONLINES,\n",
    "    model_input_location=\"dialogue\",\n",
    "    target_output_location=\"summary\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab214c0-fdb5-4678-bd13-dcee31dadde9",
   "metadata": {},
   "source": [
    "fmeval provides a `SageMakerModelRunner` class to facilitate calling the models during evaluation. The model runner, combined with the dataset, provides fmeval with the resources it needs to run through the source data file, generate model responses, then calculate the scores to compare.\n",
    "\n",
    "Note the `output` maps to `generated_text` which is the output parameter from your model, and that the 2 tuned options have references to `adapters` in their `content_template` to specify the LoRA adapter to swap in for that inference call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcac567-b927-4e77-bc74-a32e6805bf4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from fmeval.model_runners.sm_model_runner import SageMakerModelRunner\n",
    "\n",
    "base_sagemaker_model_runner = SageMakerModelRunner(\n",
    "    endpoint_name=base_endpoint_name,\n",
    "    output='generated_text',\n",
    "    content_type='application/json',\n",
    "    accept_type='application/json',\n",
    "    content_template='{\"inputs\": $prompt, \"parameters\": {\"do_sample\": true, \"top_p\": 0.9, \"temperature\": 0.9, \"max_new_tokens\": 256}}',\n",
    ")\n",
    "\n",
    "tuned_sagemaker_model_runner = SageMakerModelRunner(\n",
    "    endpoint_name=tuned_endpoint_name,\n",
    "    output='generated_text',\n",
    "    content_type='application/json',\n",
    "    accept_type='application/json',\n",
    "    content_template='{\"inputs\": $prompt, \"parameters\": {\"do_sample\": true, \"top_p\": 0.9, \"temperature\": 0.9, \"max_new_tokens\": 256}, \"adapters\":[\"sum\"]}',\n",
    ")\n",
    "\n",
    "tuned5_sagemaker_model_runner = SageMakerModelRunner(\n",
    "    endpoint_name=tuned_endpoint_name,\n",
    "    output='generated_text',\n",
    "    content_type='application/json',\n",
    "    accept_type='application/json',\n",
    "    content_template='{\"inputs\": $prompt, \"parameters\": {\"do_sample\": true, \"top_p\": 0.9, \"temperature\": 0.9, \"max_new_tokens\": 256}, \"adapters\":[\"sum5\"]}',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86721fce-b1dd-4b71-93d2-ed3c405bd28b",
   "metadata": {},
   "source": [
    "### Evaluation run configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70be995-5688-4c94-9da0-ad327b89523c",
   "metadata": {},
   "source": [
    "Out of the box, fmeval writes the reports for evaluation runs to /tmp on the filesystem. To keep those files for analysis, you will set the `EVAL_RESULTS_PATH` environment variable to a subdirectory of the workshop folder.\n",
    "\n",
    "fmeval supports a variety of [evaluation algorithms](https://github.com/aws/fmeval/tree/main/src/fmeval/eval_algorithms). In this example you will use the `SummarizationAccuracy` one, which will output METEOR, ROUGE, and BertScore Metrics\n",
    "\n",
    "For larger datasets and evaluation instance types that have sufficient memory, you can also modify the `PARALLELIZATION_FACTOR` environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f837ce1-66dd-4682-8a81-7b41d222f055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "eval_dir = \"eval-results\"\n",
    "curr_dir = os.getcwd()\n",
    "eval_results_path = os.path.join(curr_dir, eval_dir) + \"/\"\n",
    "os.environ[\"EVAL_RESULTS_PATH\"] = eval_results_path\n",
    "if not os.path.exists(eval_results_path):\n",
    "    os.mkdir(eval_results_path)\n",
    "\n",
    "os.environ[\"PARALLELIZATION_FACTOR\"] = \"8\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f863eff5-253d-45d9-82ba-d8be8766b409",
   "metadata": {},
   "source": [
    "This is the prompt template that you will use for your evaluation runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfacfe4-6cf6-4c46-ac87-4e4ab8c19c88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluation_prompt_template = \"\"\"\n",
    "<|begin_of_text|>\n",
    "<|start_header_id|>system<|end_header_id|>\n",
    "You are a helpful assistant who is an expert in summarizing conversations.\n",
    "<|eot_id|>\n",
    "\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "Summarize the provided conversation in 2 sentences.\n",
    "\n",
    "$model_input\n",
    "\n",
    "Provide the summary directly, without any introduction or preamble. Do not start the response with \"Here is a...\".<|eot_id|>\n",
    "\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56e94a1-a7d3-4290-9b19-6f5b9cb09e14",
   "metadata": {},
   "source": [
    "Next you will run 3 evaluations, 1 for each variant of your model. Ignore the verbose outputs/warnings here.\n",
    "\n",
    "For 10 items each test should take about 1-2 minutes on a `ml.m5.2xlarge` with an inference endpoint of `ml.g5.2xlarge`. The entire dataset will take 15-20 minutes per test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108c382e-2ab8-4865-9a0b-27be9349e9df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "eval_algo = SummarizationAccuracy()\n",
    "\n",
    "base_eval_output = eval_algo.evaluate(\n",
    "    model=base_sagemaker_model_runner,\n",
    "    dataset_config=base_model_data_config, \n",
    "    prompt_template=evaluation_prompt_template,\n",
    "    save=True,\n",
    "    num_records=number_of_samples_to_take\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ad09b1-e5f0-4d95-aba6-afb8f3a2104c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "eval_algo = SummarizationAccuracy()\n",
    "\n",
    "tuned_eval_output = eval_algo.evaluate(\n",
    "    model=tuned_sagemaker_model_runner,\n",
    "    dataset_config=tuned_model_data_config, \n",
    "    prompt_template=evaluation_prompt_template,\n",
    "    save=True,\n",
    "    num_records=number_of_samples_to_take\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf488ade-2bfb-4c4e-9e99-9b33e5daeb18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "eval_algo = SummarizationAccuracy()\n",
    "\n",
    "tuned5_eval_output = eval_algo.evaluate(\n",
    "    model=tuned5_sagemaker_model_runner,\n",
    "    dataset_config=tuned5_model_data_config, \n",
    "    prompt_template=evaluation_prompt_template,\n",
    "    save=True,\n",
    "    num_records=number_of_samples_to_take\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6b679c-2fa5-42e9-b090-cd11e3ad62b8",
   "metadata": {},
   "source": [
    "#### Parse Evaluation Results\n",
    "\n",
    "Now that your evaluation runs are complete, you can graph the metrics from the eval run to determine the quality of the model output. In this example you will create a histogram of the different metrics and their frequency distribution.\n",
    "\n",
    "The metrics that are part of the `SummarizationAccuracy` algorithm are:\n",
    "- **METEOR**: (**M**etric for **E**valuation of **T**ranslation with **E**xplicit **OR**dering) is an evaluation metric for machine translation that calculates the harmonic mean of unigram precision and recall, with a higher weight on recall. It also incorporates a penalty for sentences that significantly differ in length from the reference translations. The harmonic mean of unigram precision and recall provides a balanced evaluation of a machine translation system’s performance by considering both precision and recall.\n",
    "  \n",
    "- **ROUGE**: (**R**ecall-**O**riented **U**nderstudy for **G**isting **E**valuation) is an evaluation metric used to assess the quality of NLP tasks such as text summarization and machine translation. It measures the overlap of N-grams between the system-generated summary and the reference summary, providing insights into the precision and recall of the system’s output. There are several variants of ROUGE, including ROUGE-N, which quantifies the overlap of N-grams, and ROUGE-L, which calculates the Longest Common Subsequence (LCS) between the system and reference summaries.\n",
    "  \n",
    "- **BERTScore**: (**B**idirectional **E**ncoder **R**epresentations from **T**ransformers Score) is an evaluation metric for natural language processing (NLP) tasks that leverages the pre-trained BERT language model to measure the similarity between two sentences. It computes the cosine similarity between the contextualized embeddings of the words in the candidate and reference sentences. BERTScore has been shown to correlate better with human judgments and provides stronger model selection performance than existing metrics.Ï\n",
    "\n",
    "Source: [https://plainenglish.io/community/evaluating-nlp-models-a-comprehensive-guide-to-rouge-bleu-meteor-and-bertscore-metrics-d0f1b1]()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c835cb-2438-44da-8c25-a5dabfe3090a",
   "metadata": {},
   "source": [
    "First you will look at the average overall metrics for each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b6a718-85b1-4b0c-a228-903abd80bac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "colors = [\"red\", \"green\", \"blue\"]\n",
    "\n",
    "fig, axs = plt.subplots(3,1, figsize=(10,6))\n",
    "\n",
    "models = [\"Base\", \"Tuned (1 Epoch)\", \"Tuned (5 Epochs)\"]\n",
    "\n",
    "meteor_scores = [base_eval_output[0].dataset_scores[0].value, tuned_eval_output[0].dataset_scores[0].value, tuned5_eval_output[0].dataset_scores[0].value]\n",
    "rouge_scores = [base_eval_output[0].dataset_scores[1].value, tuned_eval_output[0].dataset_scores[1].value, tuned5_eval_output[0].dataset_scores[1].value]\n",
    "bert_scores = [base_eval_output[0].dataset_scores[2].value, tuned_eval_output[0].dataset_scores[2].value, tuned5_eval_output[0].dataset_scores[2].value]\n",
    "\n",
    "axs[0].bar(models, meteor_scores, color=colors)\n",
    "axs[0].set_title(\"Average METEOR Score\")\n",
    "axs[1].bar(models, rouge_scores, color=colors)\n",
    "axs[1].set_title(\"Average ROUGE Score\")\n",
    "axs[2].bar(models, bert_scores, color=colors)\n",
    "axs[2].set_title(\"Average BertScore Score\")\n",
    "\n",
    "plt.subplots_adjust(hspace=0.6)\n",
    "\n",
    "print(f\"METEOR Scores: {meteor_scores}\")\n",
    "print(f\"ROUGE Scores: {rouge_scores}\")\n",
    "print(f\"BertScore Scores: {bert_scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fb427c-c797-4e10-b06d-b6d15e91470f",
   "metadata": {},
   "source": [
    "Next you can look at small samples of each dataset. You will notice the full prompt, model output (generated from the test), target output (ground truth) and the various scores from the evaluation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e243cf-42e7-46f5-aae3-c9898e3b262c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a Pandas DataFrame to visualize the results\n",
    "import pandas as pd\n",
    "\n",
    "base_data = []\n",
    "# We obtain the path to the results file from \"output_path\" in the cell above\n",
    "with open(\"./eval-results/summarization_accuracy_base_model_samsum_sample.jsonl\", \"r\") as file:\n",
    "    for line in file:\n",
    "        base_data.append(json.loads(line))\n",
    "base_df = pd.DataFrame(base_data)\n",
    "base_df['meteor_score'] = base_df['scores'].apply(lambda x: x[0]['value'])\n",
    "base_df['rouge_score'] = base_df['scores'].apply(lambda x: x[1]['value'])\n",
    "base_df['bert_score'] = base_df['scores'].apply(lambda x: x[2]['value'])\n",
    "base_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d800c234-579e-4d95-8f5c-0612ad294da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_data = []\n",
    "# We obtain the path to the results file from \"output_path\" in the cell above\n",
    "with open(\"./eval-results/summarization_accuracy_tuned_model_samsum_sample.jsonl\", \"r\") as file:\n",
    "    for line in file:\n",
    "        tuned_data.append(json.loads(line))\n",
    "tuned_df = pd.DataFrame(tuned_data)\n",
    "tuned_df['meteor_score'] = tuned_df['scores'].apply(lambda x: x[0]['value'])\n",
    "tuned_df['rouge_score'] = tuned_df['scores'].apply(lambda x: x[1]['value'])\n",
    "tuned_df['bert_score'] = tuned_df['scores'].apply(lambda x: x[2]['value'])\n",
    "tuned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204e7b1b-fd97-40d7-a0a9-109afeaf12cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned5_data = []\n",
    "# We obtain the path to the results file from \"output_path\" in the cell above\n",
    "with open(\"./eval-results/summarization_accuracy_tuned5_model_samsum_sample.jsonl\", \"r\") as file:\n",
    "    for line in file:\n",
    "        tuned5_data.append(json.loads(line))\n",
    "tuned5_df = pd.DataFrame(tuned5_data)\n",
    "tuned5_df['meteor_score'] = tuned5_df['scores'].apply(lambda x: x[0]['value'])\n",
    "tuned5_df['rouge_score'] = tuned5_df['scores'].apply(lambda x: x[1]['value'])\n",
    "tuned5_df['bert_score'] = tuned5_df['scores'].apply(lambda x: x[2]['value'])\n",
    "tuned5_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a954e008-e645-4491-975e-95a1ded72253",
   "metadata": {},
   "source": [
    "Finally you will plot all the scores for the 3 variants. This helps to visualize the performance of one candidate versus the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a559795c-62e5-4707-8f9f-d0507892351d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "colors = [\"red\", \"green\", \"blue\"]\n",
    "\n",
    "fig, axs = plt.subplots(3,1, figsize=(10,6))\n",
    "\n",
    "base_df['meteor_score'].plot.hist(histtype=\"step\", bins=20, alpha=0.5, ax=axs[0], title = \"Meteor Score\", xlabel=\"Score\", ylabel=\"Frequency\", color=colors[0], label=\"Base\")\n",
    "base_df['rouge_score'].plot.hist(histtype=\"step\", bins=20, alpha=0.5,ax=axs[1], title=\"Rouge Score\", xlabel=\"Score\", ylabel=\"Frequency\", color=colors[0], label=\"Base\")\n",
    "base_df['bert_score'].plot.hist(histtype=\"step\", bins=20, alpha=0.5,ax=axs[2], title=\"Bert Score\", xlabel=\"Score\", ylabel=\"Frequency\", color=colors[0], label=\"Base\")\n",
    "\n",
    "tuned_df['meteor_score'].plot.hist(histtype=\"step\", bins=20, alpha=0.5, ax=axs[0], title = \"Meteor Score\", xlabel=\"Score\", ylabel=\"Frequency\", color=colors[1], label=\"Tuned 1 Epoch\")\n",
    "tuned_df['rouge_score'].plot.hist(histtype=\"step\", bins=20, alpha=0.5,ax=axs[1], title=\"Rouge Score\", xlabel=\"Score\", ylabel=\"Frequency\", color=colors[1], label=\"Tuned 1 Epoch\")\n",
    "tuned_df['bert_score'].plot.hist(histtype=\"step\", bins=20, alpha=0.5,ax=axs[2], title=\"Bert Score\", xlabel=\"Score\", ylabel=\"Frequency\", color=colors[1], label=\"Tuned 1 Epoch\")\n",
    "\n",
    "tuned5_df['meteor_score'].plot.hist(histtype=\"step\", bins=20, alpha=0.5, ax=axs[0], title = \"Meteor Score\", xlabel=\"Score\", ylabel=\"Frequency\", color=colors[2], label=\"Tuned 5 Epoch\")\n",
    "tuned5_df['rouge_score'].plot.hist(histtype=\"step\", bins=20, alpha=0.5,ax=axs[1], title=\"Rouge Score\", xlabel=\"Score\", ylabel=\"Frequency\", color=colors[2], label=\"Tuned 5 Epoch\")\n",
    "tuned5_df['bert_score'].plot.hist(histtype=\"step\", bins=20, alpha=0.5,ax=axs[2], title=\"Bert Score\", xlabel=\"Score\", ylabel=\"Frequency\", color=colors[2], label=\"Tuned 5 Epoch\")\n",
    "\n",
    "axs[0].legend(loc=\"upper right\")\n",
    "axs[1].legend(loc=\"upper right\")\n",
    "axs[2].legend(loc=\"upper right\")\n",
    "\n",
    "plt.subplots_adjust(hspace=0.6) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5735d8-0ac7-4360-9250-36df6082da80",
   "metadata": {},
   "source": [
    "If you were to run a full dataset evaluation against all 3 variants, your graph would look something like this. Notice that there is a large boost in model performance with 1 epoch of fine tuning, with somewhat diminishing returns for the 5 epoch variant.\n",
    "\n",
    "![](./overall_scores_with_details.png)\n",
    "![](./full_dataset_eval_results.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7bc906-7bb3-45cd-87c9-74cd4b92442b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
