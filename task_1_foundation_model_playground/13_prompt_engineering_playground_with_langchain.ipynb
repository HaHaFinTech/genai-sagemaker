{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41085ef9-2005-4c4f-8b3e-c15a7537f5ad",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFDDDD; border-left: 5px solid red; padding: 10px; color: black;\">\n",
    "    <strong>Kernel: Python 3 (ipykernel)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96aba234-390a-4504-8b26-dce11f7f7d37",
   "metadata": {},
   "source": [
    "## Prompt Engineering with LLMs on SageMaker Studio.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc488fb9",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Prompt engineering is an exciting, new way of making language computer programs, also known as language models, work better for all kinds of jobs and studies. This skill helps us get to know what these big computer programs can do well and what they can't.\n",
    "\n",
    "Scientists use prompt engineering to make these language models better at doing a bunch of different things, like answering questions or solving math problems. Programmers use it to create strong and useful ways to interact with these big language models and other tech stuff.\n",
    "\n",
    "But prompt engineering isn't just about making questions or commands for these models. It's a whole set of skills that help us work better with them. We can use these skills to make the language models safer and even add new features, like making them smarter in specific subjects.\n",
    "\n",
    "In this lab, we learn how to:\n",
    "1. use SageMaker to setup and send prompts to a Large Language Model, Llama 3.1.\n",
    "2. Learn basic and Advanced prompting techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f012d1e-b9e6-4fcb-a9d1-9e52e04e62c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import serializers, deserializers\n",
    "from ipywidgets import Dropdown\n",
    "from typing import Dict, List\n",
    "from IPython.display import display, HTML\n",
    "from datetime import datetime\n",
    "from langchain import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.llms import SagemakerEndpoint\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate, \n",
    "    HumanMessagePromptTemplate\n",
    ")\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "from utilities.helpers import (\n",
    "    pretty_print_html, \n",
    "    set_meta_llama_params,\n",
    "    print_dialog,\n",
    "    format_messages,\n",
    "    read_eula,\n",
    "    ContentHandlerwithTracking\n",
    ")\n",
    "\n",
    "import mlflow\n",
    "from mlflow import MlflowClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac68dcf",
   "metadata": {},
   "source": [
    "#### Connect to an Hosted `Llama 3.1 8b Instruct` Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5edd56b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_name = \"meta-llama31-8b-instruct-endpoint\" \n",
    "mlflow_experiment_name = f\"Prompt-Tracker-{datetime.now().strftime('%y%m%d')}\"\n",
    "boto_region = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cb9b63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sess = sagemaker.session.Session(\n",
    "    boto_session=boto3.Session(\n",
    "        region_name=boto_region\n",
    "    )\n",
    ")\n",
    "sm_client = boto3.client(\n",
    "    \"sagemaker\", \n",
    "    region_name=boto_region\n",
    ")\n",
    "\n",
    "smr_client = boto3.client(\n",
    "    \"sagemaker-runtime\", \n",
    "    region_name=boto_region\n",
    ")\n",
    "\n",
    "pretrained_predictor = sagemaker.Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sess,\n",
    "    serializer=serializers.JSONSerializer(),\n",
    "    deserializer=deserializers.JSONDeserializer(),\n",
    ")\n",
    "\n",
    "tracking_server_arn = sm_client.list_mlflow_tracking_servers(\n",
    ")['TrackingServerSummaries'][0]['TrackingServerArn']\n",
    "tracking_server_url = sm_client.create_presigned_mlflow_tracking_server_url(\n",
    "    TrackingServerName=tracking_server_arn.split('/')[-1]\n",
    ")\n",
    "mlflow.set_tracking_uri(tracking_server_arn)\n",
    "experiment_info = mlflow.set_experiment(mlflow_experiment_name)\n",
    "\n",
    "content_handler = ContentHandlerwithTracking(mlflow_experiment_name)\n",
    "\n",
    "llm=SagemakerEndpoint(\n",
    "     endpoint_name=pretrained_predictor.endpoint_name, \n",
    "     region_name=sess.boto_region_name, \n",
    "     model_kwargs={\n",
    "         \"max_new_tokens\": 700, \n",
    "         \"top_p\": 0.8, \n",
    "         \"temperature\": 0.1\n",
    "     },\n",
    "    content_handler=content_handler\n",
    " )\n",
    "\n",
    "pretty_print_html(f\"Using tracking server {tracking_server_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebda0a5e-8bd7-4cdf-a0de-d23a04ef9003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_llm_inference(llm, template, sanitize_output=False, repl=False, **kwargs):\n",
    "    # Clean up kwargs inputs\n",
    "    for _k in kwargs:\n",
    "        kwargs[_k] = kwargs[_k].rstrip().strip().replace('\\n', '')\n",
    "\n",
    "    # Build the base LLM chain\n",
    "    llm_chain = template | llm\n",
    "\n",
    "    # Dynamically add output sanitization if requested\n",
    "    if sanitize_output:\n",
    "        llm_chain = llm_chain | StrOutputParser() | _sanitize_output\n",
    "\n",
    "    if repl:\n",
    "        llm_chain = llm_chain | repl\n",
    "    \n",
    "    # Get the response from the chain\n",
    "    response = llm_chain.invoke(kwargs)\n",
    "    \n",
    "    # Optionally truncate long input values\n",
    "    for _k in kwargs:\n",
    "        if len(kwargs[_k]) > 200:\n",
    "            kwargs[_k] = kwargs[_k][:200] + \"...\"\n",
    "    \n",
    "    # Format and return the full message\n",
    "    if not sanitize_output and not repl:\n",
    "        full_message = template.format(**kwargs) + f\"\\nAI: {response}\"\n",
    "    elif sanitize_output and not repl:\n",
    "        full_message = template.format(**kwargs) + f\"\\nAI: \\n ```\\n{response}```\"\n",
    "    elif sanitize_output and repl:\n",
    "        full_message = template.format(**kwargs) + f\"\\nAI: \\n (executed output code) \\n{response}```\"\n",
    "        \n",
    "    return full_message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d02611-7a0f-470a-b359-81189870debc",
   "metadata": {},
   "source": [
    "The function below is used to set the inference payload parameters for `Llama` Endpoint\n",
    "\n",
    "* **max_new_tokens:** Model generates text until the output length (excluding the input context length) reaches max_new_tokens. If specified, it must be a positive integer.\n",
    "\n",
    "* **temperature:** temperature: Controls the randomness in the output. Higher temperature results in output sequence with low-probability words and lower temperature results in output sequence with high-probability words. If temperature -> 0, it results in greedy decoding. If specified, it must be a positive float.\n",
    "\n",
    "* **top_p:** Top p, also known as nucleus sampling, is another hyperparameter that controls the randomness of language model output. sets a threshold probability and selects the top tokens whose cumulative probability exceeds the threshold. The model then randomly samples from this set of tokens to generate output. This method can produce more diverse and interesting output than traditional methods that randomly sample the entire vocabulary. For example, if you set top p to **0.9**, the model will only consider the most likely words that make up **90%** of the probability mass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035e2b57-c3d8-41d9-af41-0394cfe2a55f",
   "metadata": {},
   "source": [
    "#### Start Tracking your prompts with `MLflow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25296e88-cc73-44dd-80fe-89b4ae85191b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(\n",
    "    f'Access MLflow Tracking Server here: <a href=\"{tracking_server_url[\"AuthorizedUrl\"]}\" target=\"_blank\">{tracking_server_arn.split(\"/\")[-1]}</a>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73ca8d7-3691-431c-98cd-6714a43a0177",
   "metadata": {},
   "source": [
    "## Prompt Engineering Basics\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4ba2e2-83fb-4f75-b57e-eb3f9d239ec0",
   "metadata": {},
   "source": [
    "## Basic prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac77b873-e54f-4162-9cbd-09602aba78f4",
   "metadata": {},
   "source": [
    "In this lab, we'll delve Prompt Engineering examples that showcase the utility of well-designed prompts, setting the stage for the more complex topics explored in advanced modules.<br>\n",
    "\n",
    "Understanding key principles often becomes clearer when illustrated with real-world examples. In the sections that follow, we demonstrate a variety of tasks made possible through the strategic crafting of prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8db41fd-ae8a-4459-836b-ffd24ac50789",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_template = '''You are a helpful ai assistant. Keep your answers short and talkative only when required!\n",
    "\n",
    "{text}\n",
    "'''\n",
    "\n",
    "basic_prompt_template = PromptTemplate.from_template(basic_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5ad9a9-4829-4fe9-8a95-6beb679ec474",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_response = run_llm_inference(llm=llm, text=\"The sky is\", template=basic_prompt_template)\n",
    "pretty_print_html(full_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d40ab7-5c2f-4cba-880b-9ed8916aba36",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_response = run_llm_inference(llm=llm, text=\"Translate this sentence to French: I am learning to speak French.\", template=basic_prompt_template)\n",
    "pretty_print_html(full_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01bd8b2-8444-46ff-a0ae-47f44df7982c",
   "metadata": {},
   "source": [
    "### Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f365ef5b-c024-4a30-b1a6-b50af997a621",
   "metadata": {},
   "source": [
    "One of the key activities in natural language generation involves text summarization, which comes in various forms and contexts. One of the most intriguing capabilities of language models is their skill in distilling lengthy articles or complex ideas into brief, easy-to-grasp summaries. For this exercise, we will delve into the basics of text summarization using tailored prompts.\n",
    "\n",
    "Suppose you wish to familiarize yourself with the age-old tale of \"The Tortoise and the Hare.\" You could start with a prompt like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238ad230-e17d-4f6c-86ac-3f217df35249",
   "metadata": {},
   "outputs": [],
   "source": [
    "summ_template = '''You are a helpful ai assistant. Summarize the paragraph below in one sentence only:\n",
    "\n",
    "{text}\n",
    "'''\n",
    "\n",
    "summ_prompt_template = PromptTemplate.from_template(summ_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fb30e4-968c-4c1f-a446-4545c59ea42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_response = run_llm_inference(llm=llm, text=\"\"\"The hare was once boasting of his speed before the other animals. \"I have never yet been beaten,\" said he, \"when I put forth my full speed. I challenge any one here to race with me.\" The tortoise said quietly, \"I accept your challenge.\" \"That is a good joke,\" said the hare; \"I could dance round you all the way.\" \"Keep your boasting till you've beaten me,\" answered the tortoise. \"Shall we race?\" So a course was fixed and a start was made. The hare darted almost out of sight at once, but soon stopped and, to show his contempt for the tortoise, lay down to have a nap. The tortoise plodded on and plodded on, and when the hare awoke from his nap, he saw the tortoise just near the winning-post and could not run up in time to save the race.\"\"\", template=summ_prompt_template)\n",
    "pretty_print_html(full_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4bc325-6fce-4c36-9e43-5c7c83be0ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"https://blog.langchain.dev/langgraph/\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404ae2cd-ad7e-493d-a022-b7519575157e",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_response = run_llm_inference(llm=llm, text=docs[0].page_content, template=summ_prompt_template)\n",
    "pretty_print_html(full_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6899e56b-023a-40c4-9af3-b864b38bfe1c",
   "metadata": {},
   "source": [
    "### Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3542361-133e-47eb-a69a-5c98c584bf98",
   "metadata": {},
   "source": [
    "A highly effective method for eliciting precise responses from the model involves refining the structure of the prompt. As previously discussed, a well-designed prompt often amalgamates elements like directives, contextual information, and input-output indicators to yield superior outcomes. While incorporating these elements isn't obligatory, doing so tends to be advantageous; specificity in your instructions is directly correlated with the quality of the results you obtain. The subsequent section offers an illustrative example to demonstrate the impact of a meticulously crafted prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cf98d2-005f-4282-9f97-a8e89b811713",
   "metadata": {},
   "outputs": [],
   "source": [
    "qna_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=(\n",
    "                \"Answer the following question based on the context below. Keep the answer short. Respond 'Unsure about answer' if not sure about the answer.\"\n",
    "            )\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"Context: {context_text}\\nQuestion:{context_q}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a31c29e-2105-40ce-9c80-111666fc2ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_text = \"In 1849, thousands of people rushed to California in search of gold and riches. This was known as the California Gold Rush. Prospectors came from all over the world during this time period.\"\n",
    "context_q = \"What year did the events take place?\"\n",
    "\n",
    "full_response = run_llm_inference(llm=llm, context_text=context_text, context_q=context_q, template=qna_prompt_template)\n",
    "\n",
    "pretty_print_html(full_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59380670-1348-4de6-af00-e40c27c7ab1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/features.html\")\n",
    "docs = loader.load()\n",
    "\n",
    "context_q = \"What is Amazon Q?\"\n",
    "full_response = run_llm_inference(llm=llm, context_text=docs[0].page_content, context_q=context_q, template=qna_prompt_template)\n",
    "pretty_print_html(full_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8d0d00-3fe6-45c6-934d-30c9752d4b86",
   "metadata": {},
   "source": [
    "### Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e06fff-bcf7-495d-ae48-5ea4701acdc9",
   "metadata": {},
   "source": [
    "Up to this point, you've given straightforward directives to achieve specific outcomes. However, in your role as a prompt engineer, enhancing the quality of your instructions is imperative. It's not just about better commands; for more complex scenarios, mere instructions won't suffice. This is the juncture where contextual understanding and nuanced elements become crucial. Elements such as [input data] or illustrative [examples] can offer further guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414087f9-801e-4ff5-bcfc-65ba0a31c85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_template = '''Classify the text into negative or positive.\n",
    "\n",
    "Text: {context_text}\n",
    "\n",
    "Sentiment:'''\n",
    "\n",
    "sentiment_prompt_template = PromptTemplate.from_template(sentiment_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145275f2-6ce7-4cf0-a122-442b21e0e605",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_text = \"Apple stock is currently trading at 150 dollars per share. Given Apple's strong financial performance lately with increased iPhone sales and new product launches planned, I predict the stock price will increase to around 160 dollars per share over the next month.\"\n",
    "full_response = run_llm_inference(llm=llm, context_text=context_text, template=sentiment_prompt_template)\n",
    "pretty_print_html(full_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a2e708-f6c9-419d-aa2b-790bb7539e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_template = '''Determine if this article is about \"Technology\", \"Politics\", or \"Business\".\n",
    "\n",
    "Text: {context_text}\n",
    "\n",
    "Category:'''\n",
    "\n",
    "sentiment_prompt_template = PromptTemplate.from_template(sentiment_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7244bc8-a74b-4699-a795-b02e5cdfae20",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_text = \"The article discussed how social media platforms like Facebook and Twitter are dealing with harmful content and political misinformation leading up to the next US presidential election.\"\n",
    "full_response = run_llm_inference(llm=llm, context_text=context_text, template=sentiment_prompt_template)\n",
    "pretty_print_html(full_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052b3b76-3a16-44a8-b185-829e7f7fabdd",
   "metadata": {},
   "source": [
    "### Role Playing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76077176-8387-438a-9493-4cb6e4c4470d",
   "metadata": {},
   "outputs": [],
   "source": [
    "role_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an AI research assistant. Your tone is technical and scientific. Your name is {name}. Keep your answers less than 5 sentences.\"),\n",
    "        (\"human\", \"Hello, how are you doing and who are you?\"),\n",
    "        (\"ai\", \"Greeting! I am an AI research assistant named {name}. I am doing well! How can I help you today?\"),\n",
    "        (\"human\", \"{text}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfa4c88-63b3-4d28-b415-9b832deea076",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Can you tell me about the creation of volcanic mountains?\"\n",
    "full_response = run_llm_inference(llm=llm, name=\"Jarvis\", text=text, template=role_prompt_template)\n",
    "pretty_print_html(full_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ef73a5-f0d9-4f02-98e3-eb8e63cc3b6f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Code Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb87c06b-86c6-4f35-ac9c-546ea59d7e76",
   "metadata": {},
   "source": [
    "Code generation involves prompting a model to generate code without providing any examples, relying solely on the model's pre-training. We will test this by giving the model instructions to write code snippets without any demonstrations.\n",
    "\n",
    "The model will attempt to produce valid code based on these descriptions using its implicit knowledge gained during training.\n",
    "\n",
    "Evaluating zero-shot code generation will allow us to test the limits of the model's unaided coding skills for different languages. The results can reveal strengths, gaps, and opportunities to supplement with few-shot examples.\n",
    "\n",
    "This section will provide insights into current capabilities and future work needed to move toward general-purpose AI that can code without extensive training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f45fd3-202c-4c1a-8a10-f05d19641038",
   "metadata": {},
   "source": [
    "#### Python code generation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636c4657-30b7-49ae-adc3-acdea855e7a2",
   "metadata": {},
   "source": [
    "For Python, we can provide prompts like:\n",
    "- Write a Python function that prints numbers from 1 to 10\n",
    "- Generate Python code to open a file and read the contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becb7e19-833d-458d-8c4b-ae4c0e567a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sanitize_output(text: str):\n",
    "    if 'java' in text:\n",
    "        _, after = text.split(\"```java\")\n",
    "    elif 'python' in text:\n",
    "        _, after = text.split(\"```python\")\n",
    "    return after.split(\"```\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83809415-4a1f-4b37-8af4-6a69d6bcf368",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_template = \"\"\"Write some python code to solve the user's problem. \n",
    "\n",
    "Return only python code in Markdown format, e.g.:\n",
    "\n",
    "```python\n",
    "....\n",
    "```\"\"\"\n",
    "code_prompt_template = ChatPromptTemplate.from_messages([(\"system\", code_template), (\"human\", \"{code_text}\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a296680-0c2a-41aa-adab-b1fe43ce9e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_text = \"Download a file from s3://xyz/file/sample.xyz from Amazon S3 to local disk path ./\"\n",
    "\n",
    "full_response = run_llm_inference(\n",
    "    llm=llm, code_text=code_text, template=code_prompt_template, sanitize_output=_sanitize_output, repl=None\n",
    ")\n",
    "\n",
    "pretty_print_html(full_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1068d2ed-84d1-4f42-9e19-0e8e52df8d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain = prompt | llm | StrOutputParser() | _sanitize_output  \n",
    "code_text = \"Generate a Python program that prints the numbers from 1 to 10\"\n",
    "full_response = run_llm_inference(\n",
    "    llm=llm, code_text=code_text, template=code_prompt_template, \n",
    "    sanitize_output=_sanitize_output, repl=PythonREPL().run\n",
    ")\n",
    "pretty_print_html(full_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98204cf9-14f4-4ad7-a8c9-26521bf1e587",
   "metadata": {},
   "source": [
    "#### JavaScript code generation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7aace4d-657a-42d9-9b46-cb7bbd8f4b7d",
   "metadata": {},
   "source": [
    "For JavaScript:\n",
    "- Write a JavaScript function that returns the maximum value in an array\n",
    "- Generate JavaScript code to create a for loop that prints numbers 1 to 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20892312-8322-4455-bf6d-0dcd8ae426f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_template = \"\"\"Write some java code to solve the user's problem. \n",
    "\n",
    "Return only java code in Markdown format, e.g.:\n",
    "\n",
    "```java\n",
    "....\n",
    "```\"\"\"\n",
    "code_prompt_template = ChatPromptTemplate.from_messages([(\"system\", code_template), (\"human\", \"{code_text}\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2b7985-419d-4529-8e37-5d3fda0eaff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_text = \"Write a JavaScript function that returns the largest number in an array\"\n",
    "\n",
    "full_response = run_llm_inference(llm=llm, code_text=code_text, template=code_prompt_template, sanitize_output=_sanitize_output)\n",
    "\n",
    "pretty_print_html(full_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b55655-43f0-4139-b2a3-7259440c917c",
   "metadata": {},
   "source": [
    "#### SQL Code generation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b65f363-d84b-4b14-a6eb-eb078a97ef41",
   "metadata": {},
   "source": [
    "For SQL:\n",
    "- MySQL query to get the title and quantity for all books where the quantity is greater than 100\n",
    "- Generate SQL code to join the \"orders\" and \"products\" tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18328eea-69ed-4824-9a02-229304606d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_template = \"\"\"\n",
    "{tables_text}\n",
    "\n",
    "Create a MySQL query to get the title and quantity for all books where the quantity is greater than 100 and explain the query in no more than 100 words.\n",
    "\"\"\"\n",
    "\n",
    "sql_prompt_template = PromptTemplate.from_template(sql_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0cfbf1-e9d3-49e2-b457-0ae2062b1892",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_text = \"\"\"\n",
    "Table books, columns = [BookId, Title, Author]\n",
    "Table inventory, columns = [BookId, Quantity]\n",
    "\"\"\"\n",
    "\n",
    "full_response = run_llm_inference(\n",
    "    llm=llm, tables_text=tables_text, template=sql_prompt_template, sanitize_output=None, repl=None\n",
    ")\n",
    "\n",
    "pretty_print_html(full_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d84d67-278e-4d7f-8125-9c34537b9850",
   "metadata": {},
   "source": [
    "### Reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850f7deb-1a50-428b-a73d-157789b7a363",
   "metadata": {},
   "source": [
    "Today, one of the most formidable challenges for Large Language Models (LLMs) lies in the domain of reasoning. This area intrigues me significantly, given the intricate applications that could benefit from enhanced reasoning capabilities in LLMs.\n",
    "\n",
    "While there have been strides in the model's mathematical functionalities, it's crucial to underscore that tasks involving reasoning are often stumbling blocks for existing LLMs. Specialized techniques in prompt engineering are imperative to navigate these challenges. While we'll delve into these advanced strategies in an upcoming guide, this lab will provide a primer by walking you through basic examples that demonstrate the model's capabilities in deductive reasoning and logical inferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac822d2-1232-452a-b9d5-fb754e06959c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=(\n",
    "                \"Given the facts, Use logical reasoning to conclude.\"\n",
    "            )\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"Given Fact: {fact_text}\\nExplain your reasoning: {explain_text}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eef08ad-e4e2-4fd2-89d0-79a12ba777ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_text = \"All men are mortal. Socrates is a man.\"\n",
    "explain_text = \"Is Socrates mortal?\"\n",
    "\n",
    "full_response = run_llm_inference(llm=llm, fact_text=fact_text, explain_text=explain_text, template=reasoning_prompt_template)\n",
    "\n",
    "pretty_print_html(full_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227dcfa0-0456-420a-acad-b402ac1d4108",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_text = \"There are 5 people - Alan, Beth, Cindy, David and Erica. Alan is taller than Beth. Beth is shorter than Cindy. Cindy is taller than David. David is taller than Erica.\"\n",
    "explain_text = \"Who is the tallest?\"\n",
    "\n",
    "full_response = run_llm_inference(llm=llm, fact_text=fact_text, explain_text=explain_text, template=reasoning_prompt_template)\n",
    "\n",
    "pretty_print_html(full_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301bd521-5eb0-4c44-b218-11aa057f7af3",
   "metadata": {},
   "source": [
    "## Advanced Prompting Techniques\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fee61ed-6cbf-4829-81dd-f27a188af26a",
   "metadata": {},
   "source": [
    "### Zero-shot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decdf6dd-794d-49ea-a27a-e909d3095c51",
   "metadata": {},
   "source": [
    "Modern large language models like Llama have been optimized to follow instructions and trained on enormous datasets. This enables them to perform certain tasks without any fine-tuning, known as zero-shot learning. Previously, we evaluated some zero-shot prompts. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741078eb-fecd-4bb6-a600-f46918bfaa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "zs_template = \"\"\"\n",
    "Translate this sentence to {language}: I am learning to speak {language}.\n",
    "\"\"\"\n",
    "\n",
    "zs_prompt_template = PromptTemplate.from_template(zs_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2bed2a-3648-4a62-a666-3aef6e0c4b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "language = \"French\"\n",
    "\n",
    "full_response = run_llm_inference(\n",
    "    llm=llm, language=language, template=zs_prompt_template\n",
    ")\n",
    "\n",
    "pretty_print_html(full_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbe2dfa-e4b4-4305-a557-f324970d92d6",
   "metadata": {},
   "source": [
    "Although Large Language Models (LLMs) exhibit impressive abilities in zero-shot scenarios, their performance can falter when tackling more intricate tasks within that context. To ameliorate this, the concept of few-shot prompting comes into play. This technique facilitates in-context learning by incorporating example-based guidance directly into the prompt, thereby enhancing the model's output accuracy. These examples act as a form of conditioning that influences the model's responses in subsequent instances.\n",
    "\n",
    "For illustrative purposes, let's delve into a hands-on example of few-shot prompting. In this exercise, the objective is to accurately incorporate a novel term into a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d13399-340a-4120-9e73-81de1f5e0cb7",
   "metadata": {},
   "source": [
    "### Few-shot prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057a326a-af7c-4d89-8ba7-eda4ed78967d",
   "metadata": {},
   "source": [
    "Though large language models can perform impressively without training, their zero-shot abilities still have limitations on more difficult tasks. Few-shot prompting enhances in-context learning by supplying model demonstrations directly in the prompt. These examples provide conditioning to guide the model's response for new inputs. As described by [Brown et al. (2020)](https://arxiv.org/abs/2005.14165), few-shot prompting can be applied to tasks like properly using novel words in sentences. With just a couple demonstrations, the model can acquire new concepts and skills without full training. This technique harnesses the few-shot capabilities of large models to achieve greater generalization and reasoning from small amounts of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace4caf0-1f82-4ead-95e2-6273fe1402c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"statement\": \"A 'blicket' is a tool used for farming\",\n",
    "        \"example_sentence_usage\": \"The farmer used a blicket to dig holes and plant seeds.\",\n",
    "    },\n",
    "    {\n",
    "        \"statement\": \"'Flooping' refers to a dance move where you spin around\",\n",
    "        \"example_sentence_usage\": \"The kids were flooping in circles at the dance party.\",\n",
    "    },\n",
    "    {\n",
    "        \"statement\": \"A 'zindle' is a small, magical creature that loves to collect shiny objects\",\n",
    "        \"example_sentence_usage\": \"The zindle scurried across the floor, gathering coins and jewelry in its tiny hands.\",\n",
    "    },\n",
    "    {\n",
    "        \"statement\": \"'Quizzle' means to solve a puzzle in a very creative way\",\n",
    "        \"example_sentence_usage\": \"After hours of thinking, she managed to quizzle her way out of the tricky math problem.\",\n",
    "    }\n",
    "]\n",
    "\n",
    "# \"Splonk\" describes the sound something makes when it falls into water. An example of a sentence that uses the word splonk is:\n",
    "# The rock made a loud splonk as it dropped into the pond.\n",
    "\n",
    "template_format = \"\"\"\n",
    "Statemenent: {statement}\n",
    "Sample Usage: {example_sentence_usage}\n",
    "\"\"\"\n",
    "\n",
    "few_shot_prompt_template = PromptTemplate(\n",
    "    input_variables=[\n",
    "        \"statement\", \n",
    "        \"example_sentence_usage\"\n",
    "    ], \n",
    "    template=template_format\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8259fbe-086f-473c-8855-530a7cb3eaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"The following are set of example usage of made up terms. Read the latest statement and provide an example usage.\"\n",
    "# and the suffix our user input and output indicator\n",
    "suffix = \"\"\"\n",
    "Statemenent: {statement}\n",
    "Sample Usage: \"\"\"\n",
    "\n",
    "# now create the few shot prompt template\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=few_shot_prompt_template,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5b430e-e6e7-49f3-9897-59947c1776d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "statement = \"'Splonk' describes the sound something makes when it falls into water.\"\n",
    "\n",
    "full_response = run_llm_inference(\n",
    "    llm=llm, statement=statement, template=few_shot_prompt_template\n",
    ")\n",
    "\n",
    "pretty_print_html(full_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70a14a6-31de-4e79-852f-feafe6c024c8",
   "metadata": {},
   "source": [
    "You'll notice that the LLM has the ability to grasp the task with just a single example, commonly known as 1-shot learning. For tasks that are more challenging, the lab allows you to incrementally scale the number of examples or \"shots\" (such as 3-shot, 5-shot, or even 10-shot) to experiment with improving the model's performance. Below we leveage a 5-shot Few-shot prompt to create a short story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e467ee5c-58c9-4b4d-9872-d6497c71b922",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"example\": \"A 'blonset' is a tool used for cutting metal.\"\n",
    "    },\n",
    "    {\n",
    "        \"example\": \"A 'fendle' is a vegetable that grows underground.\"\n",
    "    },\n",
    "    {\n",
    "        \"example\": \"A 'Vixting' means climbing a tree very quickly.\"\n",
    "    },\n",
    "    {\n",
    "        \"example\": \"A 'Zugging' refers to a sport played with a small ball.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# \"Splonk\" describes the sound something makes when it falls into water. An example of a sentence that uses the word splonk is:\n",
    "# The rock made a loud splonk as it dropped into the pond.\n",
    "\n",
    "template_format = \"Made-up tool: {example}\"\n",
    "\n",
    "few_shot_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"example\"], \n",
    "    template=template_format\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b41b47-7e64-4084-9e71-8d9eb13e0729",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"The following are set of example usage of made up terms. Read and respond to a user's request.\"\n",
    "\n",
    "# and the suffix our user input and output indicator\n",
    "suffix = \"\"\"\n",
    "Query: {query}\n",
    "Answer: \"\"\"\n",
    "\n",
    "# now create the few shot prompt template\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=few_shot_prompt_template,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa198c33-bdc3-4d05-a6f5-57274c215f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Create a short story that uses all 4 words:\"\n",
    "\n",
    "full_response = run_llm_inference(\n",
    "    llm=llm, query=query, template=few_shot_prompt_template\n",
    ")\n",
    "\n",
    "pretty_print_html(full_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e840c09-62cc-4c67-972a-b6c391d608aa",
   "metadata": {},
   "source": [
    "### Chain-of-Thought (CoT) Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f86b2ee-1f83-4e30-80a8-221901b2028e",
   "metadata": {},
   "source": [
    "Chain-of-thought (CoT) prompting, proposed by [Wei et al. (2022)](https://arxiv.org/abs/2201.11903), is a technique that facilitates complex reasoning in language models by having them show intermediate steps. This method can be used with few-shot prompting, where just a few examples provide the context. The combination enables improved performance on challenging tasks that involve reasoning through multiple steps before generating a final response. By eliciting the reasoning process explicitly, CoT prompting aims to develop stronger logical thinking and rationality in language models when applying them to complex inferential problems with limited data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2812f0d5-7cad-4ce2-ac1f-c9b52ed2000d",
   "metadata": {},
   "source": [
    "### Zero-shot CoT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed755b9b-78fc-4693-9eaa-0ba6a9f5481f",
   "metadata": {},
   "source": [
    "A new approach called zero-shot chain-of-thought prompting was recently proposed by [Kojima et al. (2022)](https://arxiv.org/abs/2205.11916). This technique involves adding the phrase \"Let's think step by step\" to prompts to encourage the model to show its reasoning. We can test this method on a simple problem to see how well the model explains its logical thinking process. By explicitly cueing the model to demonstrate step-by-step reasoning, zero-shot chain-of-thought prompting aims to improve transparency and understandability without requiring training on reasoning demonstrations. This emerging technique represents an interesting way to potentially enhance rationality in large language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d85c2a5-444c-4332-ab0d-2edc4f8be918",
   "metadata": {},
   "outputs": [],
   "source": [
    "zscot_template = \"\"\"Let's think step-by-step.\n",
    "If a standard deck of 52 playing cards has 4 suits (Hearts, Diamonds, Clubs and Spades) with 13 cards in each suit, how many total face cards (Jack, Queen, King) are there?\n",
    "Please demonstrate the reasoning.\n",
    "A:\n",
    "\"\"\"\n",
    "\n",
    "zscot_prompt_template = PromptTemplate.from_template(zscot_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0546bd10-2991-4389-a346-37240c10bb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_response = run_llm_inference(llm=llm, template=zscot_prompt_template)\n",
    "\n",
    "pretty_print_html(full_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bec1030-33cf-4645-a3ec-380271e08a1f",
   "metadata": {},
   "source": [
    "### Few-Shot CoT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc675e3-957c-4405-a43b-0622581ca72d",
   "metadata": {},
   "source": [
    "Few-shot Chain of Thought (CoT) prompting is a technique that combines few-shot learning with intermediate reasoning steps. In few-shot learning, just a small number of examples or \"shots\" are provided to the model to demonstrate the desired behavior. Chain-of-thought prompting has the model show its step-by-step reasoning process explicitly.\n",
    "\n",
    "In Few-shot CoT, we give the model a couple of examples that demonstrate both the target skill and the reasoning chain. This provides the model with the context needed to apply similar skills and reasoning processes to new situations.\n",
    "\n",
    "In the example below we show the model a example of a multi-step math problem with reasoning steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042fd29d-f1b2-42db-af6c-29f16507c207",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"step\": \"1) Originally there were 6 oranges\"\n",
    "    },\n",
    "    {\n",
    "        \"step\": \"2) 4 oranges were peeled\"\n",
    "    },\n",
    "    {\n",
    "        \"step\": \"3) So there must be 6 - 4 = 2 unpeeled oranges left\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# \"Splonk\" describes the sound something makes when it falls into water. An example of a sentence that uses the word splonk is:\n",
    "# The rock made a loud splonk as it dropped into the pond.\n",
    "\n",
    "template_format = \"\"\"\n",
    "Step: {step}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "few_shot_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"step\"], \n",
    "    template=template_format\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa8f035-936a-4240-9481-44ed323ba04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"\"\"\n",
    "Let's think through this.\n",
    "\n",
    "If there were 6 oranges originally and 4 were peeled, how many unpeeled oranges are left?\n",
    "\"\"\"\n",
    "\n",
    "# and the suffix our user input and output indicator\n",
    "suffix = \"\"\"\n",
    "\n",
    "Let's think step-by-step:\n",
    "\n",
    "Query: {query}\n",
    "Answer: \"\"\"\n",
    "\n",
    "# now create the few shot prompt template\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=few_shot_prompt_template,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8994611-fa66-48a8-bc90-7ae0e981bf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"If David had 9 cakes and ate 4 of them, how many are left?\"\n",
    "\n",
    "full_response = run_llm_inference(\n",
    "    llm=llm, query=query, template=few_shot_prompt_template\n",
    ")\n",
    "\n",
    "pretty_print_html(full_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd5b439-7ce5-4f15-9854-a06324fa2637",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
