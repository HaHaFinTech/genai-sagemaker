{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2468cd2f-ba8f-47c3-aa35-f703bd7b7bec",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFDDDD; border-left: 5px solid red; padding: 10px; color: black;\">\n",
    "    <strong>Kernel:</strong> Python 3 (ipykernel)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef0c804-1b42-4b4d-958b-1dfb33c483f4",
   "metadata": {},
   "source": [
    "# Deploy a LLaMA 3.1 8B Instruct Model Adapter Using SageMaker Endpoints and SageMaker Large Model Inference (LMI) Container with the SageMaker Python SDK "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898a5386-536a-4977-9ffc-988519db65d3",
   "metadata": {},
   "source": [
    "In this example you will deploy a trained adapter of `LLaMA-3.1-8B-instruct`, to a SageMaker Managed Endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5203b9e8-e332-4629-881a-476c98a8eb44",
   "metadata": {},
   "source": [
    "Update the sagemaker SDK to the latest version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6796a3-cb84-44ad-863d-48fab445d543",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -Uq sagemaker==2.232.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cfcdf8-2b3e-41d8-8dab-0a0f93e24923",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#temp fix for a bug in SM Studio with ffspec-2023 not being properly updated\n",
    "export SITE_PACKAGES_FOLDER=$(python3 -c \"import sysconfig; print(sysconfig.get_paths()['purelib'])\")\n",
    "rm -rf $SITE_PACKAGES_FOLDER/fsspec-2023*\n",
    "\n",
    "echo \"ffspec-2023 bug fix run successfully\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b969d8df-2897-4598-8a66-ed73f103059a",
   "metadata": {},
   "source": [
    "Baseline setup. Create clients for the boto3 SDK and default values for setting up the bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c7e0ab-084f-47fe-8638-bf3028d56a47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, boto3\n",
    "import sagemaker\n",
    "\n",
    "from sagemaker.model import Model\n",
    "from sagemaker import ModelPackage\n",
    "import time\n",
    "from datasets import load_dataset\n",
    "from pprint import pprint\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "sm_client = boto3.client('sagemaker')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6107bede-071b-433c-972d-ddbf9b00e47b",
   "metadata": {},
   "source": [
    "We will get the downloaded snapshot of base model from HuggingFace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e9c20a-57e1-49fa-87b3-7e70a3f8b794",
   "metadata": {},
   "source": [
    "## Large Model Inference (LMI) Containers\n",
    "\n",
    "In this example you will deploy your model using [SageMaker's Large Model Inference (LMI) Containers](https://docs.djl.ai/master/docs/serving/serving/docs/lmi/index.html).\n",
    "\n",
    "LMI containers are a set of high-performance Docker Containers purpose built for large language model (LLM) inference. With these containers, you can leverage high performance open-source inference libraries like vLLM, TensorRT-LLM, Transformers NeuronX to deploy LLMs on AWS SageMaker Endpoints. These containers bundle together a model server with open-source inference libraries to deliver an all-in-one LLM serving solution.\n",
    "\n",
    "The LMI container supports a variety of different backends, outlined in the table below. \n",
    "\n",
    "The model for this example can be deployed using the vLLM backend, which corresponds to the `djl-lmi` container image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8660ac53-d3db-4464-ac58-9440b5cc70f8",
   "metadata": {},
   "source": [
    "| Backend | SageMakerDLC | Example URI |\n",
    "| --- | --- | --- |\n",
    "|vLLM|djl-lmi|763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n",
    "|lmi-dist|djl-lmi|763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n",
    "|hf-accelerate|djl-lmi|763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n",
    "|tensorrt-llm|djl-tensorrtllm|763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.29.0-tensorrtllm0.11.0-cu124\n",
    "|transformers-neuronx|djl-neuronx|763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.29.0-neuronx-sdk2.19.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78440331-00b0-4516-9538-cdd155d41e21",
   "metadata": {},
   "source": [
    "In the following steps you will leverage the SageMaker Python SDK to build your model configuration and deploy it to SageMaker endpoint. There are alternative methods to do this as well, such as the Boto3 SDK, but the SM Python SDK reduces the amount of code necessary perform the same activities.\n",
    "\n",
    "The first step in model deployment is to [create a SageMaker Model object](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateModel.html). This consists of a unique name, a container image, and the environment configuration from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833d848a-8708-414b-be1e-676f43d05a88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store -r model_package_arn\n",
    "\n",
    "model_package_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b38a74-049b-46b0-9258-20179f18075d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "timestamp = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "endpoint_name=f\"endpoint-llama3-8b-instruct-adapter-{timestamp}\"\n",
    "\n",
    "\n",
    "# Add your Hugging Face token only if you using base moded id from HF. Skip it for the workshop\n",
    "hf_env={\n",
    "        \"HF_TOKEN\": \"\",  # Add your Hugging Face token here\n",
    "    }\n",
    "\n",
    "model_config={\n",
    "    \"model_package_arn\":model_package_arn,\n",
    "    \"sagemaker_session\":sess,\n",
    "    \"role\":role,\n",
    "    \"name\":endpoint_name,\n",
    "    \"env\":{}\n",
    "}\n",
    "\n",
    "#model_config[\"env\"].update(hf_env)\n",
    "\n",
    "print(model_config)\n",
    "\n",
    "ft_model = ModelPackage(\n",
    " **model_config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a15e14-dd62-4b7a-8e3d-2f5bc6bcff14",
   "metadata": {},
   "source": [
    "Now that you have a model object ready, you will use use the SageMaker Python SDK to create a SageMaker Managed Endpoint. The SDK eliminates some of the intermediate steps, such as creating an Endpoint Configuration.\n",
    "\n",
    "## Creating a new endpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dda03d-4d8d-4a48-b35d-b2799c2d7d1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(f\"Deploying model with endpoint name ep-{ft_model.name}\")\n",
    "ft_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g5.2xlarge\",\n",
    "    endpoint_name=f\"{ft_model.name}\",\n",
    "    container_startup_health_check_timeout=900,\n",
    "    wait=False\n",
    ")\n",
    "print(f\"\\nEndpoint deployed ===>\", ft_model.endpoint_name)\n",
    "\n",
    "sess.wait_for_endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0060d16e-5135-48de-830a-2700b9701b6b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pick a random prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b1168f-c3b0-4ec8-b66c-707833592b28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_summarization_prompts(data_point):\n",
    "    full_prompt =f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "                    You are an AI assistant trained to summarize conversations. Provide a concise summary of the dialogue, capturing the key points and overall context.\n",
    "                    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "                    Summarize the following conversation:\n",
    "\n",
    "                    {data_point[\"dialogue\"]}\n",
    "                    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "                    Here's a concise summary of the conversation in a single sentence:\n",
    "\n",
    "                    <|eot_id|>\"\"\"\n",
    "    return {\"prompt\": full_prompt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d579721-50a4-4156-b663-0a60e1227280",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HF dataset that we will be working with \n",
    "dataset_name=\"Samsung/samsum\"\n",
    "    \n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(dataset_name, split=\"test\")\n",
    "\n",
    "random_row = dataset.shuffle().select(range(1))[0]\n",
    "\n",
    "random_prompt=create_summarization_prompts(random_row)\n",
    "pprint(random_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daf1e3c-7338-4ff7-8a8b-8bccaf8711b2",
   "metadata": {},
   "source": [
    "# Run Inference\n",
    "\n",
    "With your endpoint successfully deployed, you will want to test it to ensure that it is fully functional.\n",
    "\n",
    "To do so, you will take a piece of sample text and summarize it using your deployed model. This sample text was pulled from the [ECTSum dataset](https://huggingface.co/datasets/mrSoul7766/ECTSum)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28c4f02-5ed9-464c-8e42-bf2fc06e0656",
   "metadata": {},
   "source": [
    "Using the sample article and prompt template, invoke the model to view the structure of the response and its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6256f9f-6885-4091-9c61-4eacb536488c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "\n",
    "#endpoint_name = \"\"\n",
    "print(endpoint_name)\n",
    "\n",
    "llm = sagemaker.Predictor(\n",
    "    endpoint_name = endpoint_name,\n",
    "    sagemaker_session = sess,\n",
    "    serializer = sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer = sagemaker.deserializers.JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155ff092-a144-40d1-8d2a-a41d7776409b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = llm.predict(\n",
    "    {\n",
    "        \"inputs\": random_prompt['prompt'],\n",
    "        \"parameters\": {\n",
    "            \"do_sample\":True,\n",
    "            \"max_new_tokens\":200,\n",
    "            \"top_p\":0.95,\n",
    "            \"top_k\":50,\n",
    "            \"temperature\":0.7\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "response['generated_text']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
