{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFDDDD; border-left: 5px solid red; padding: 10px; color: black;\">\n",
    "    <strong>Kernel: Python 3 (ipykernel)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune Llama 3.1 with PyTorch FSDP and Q-Lora on Amazon SageMaker\n",
    "\n",
    "This blog post explains how you can fine-tune a Llama 3.1 8b model using PyTorch FSDP and Q-Lora with the help of Hugging Face [TRL](https://huggingface.co/docs/trl/index), [Transformers](https://huggingface.co/docs/transformers/index), [peft](https://huggingface.co/docs/peft/index) & [datasets](https://huggingface.co/docs/datasets/index) on Amazon SageMaker. \n",
    "\n",
    "**This notebook is validated and optimized to run on `ml.p3.2xlarge` instances**\n",
    "\n",
    "**FSDP + Q-Lora Background**\n",
    "\n",
    "Hugging Face share the support of Q-Lora and PyTorch FSDP (Fully Sharded Data Parallel). FSDP and Q-Lora allows you now to fine-tune Llama-like architectures or Mixtral 8x7B. Hugging Face PEFT is were the core logic resides, read more about it in the [PEFT documentation](https://huggingface.co/docs/peft/v0.10.0/en/accelerate/fsdp).\n",
    "\n",
    "* [PyTorch FSDP](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) is a data/model parallelism technique that shards model across GPUs, reducing memory requirements and enabling the training of larger models more efficiently​​​​​​.\n",
    "* Q-LoRA is a fine-tuning method that leverages quantization and Low-Rank Adapters to efficiently reduced computational requirements and memory footprint. \n",
    "\n",
    "This blog post walks you thorugh how to fine-tune open LLMs from Hugging Face using Amazon SageMaker.\n",
    "\n",
    "## 1. Setup Development Environment\n",
    "\n",
    "Our first step is to install Hugging Face Libraries we need on the client to correctly prepare our dataset and start our training/evaluations jobs. Ignore this line if you've already run task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -Uq py7zr==0.22.0\n",
    "%pip install -Uq datasets==2.21.0\n",
    "%pip install -Uq transformers==4.45.0\n",
    "%pip install -Uq peft==0.12.0\n",
    "%pip install -Uq s3fs==2024.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#temp fix for a bug in SM Studio with ffspec-2023 not being properly updated\n",
    "export SITE_PACKAGES_FOLDER=$(python3 -c \"import sysconfig; print(sysconfig.get_paths()['purelib'])\")\n",
    "rm -rf $SITE_PACKAGES_FOLDER/fsspec-2023*\n",
    "\n",
    "echo \"ffspec-2023 bug fix run successfully\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from datasets import load_dataset\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import matplotlib.pyplot as plt\n",
    "from sagemaker.s3 import S3Downloader\n",
    "import os\n",
    "from sagemaker.model import Model\n",
    "\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "sagemaker_session_bucket=None\n",
    "\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HF dataset that we will be working with \n",
    "dataset_name=\"Samsung/samsum\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#This parameter will toggle between local mode (downloading from S3) and loading models from the HF Model Hub.\n",
    "#In a workshop environment you will have a local model pre-downloaded. \n",
    "#Otherwise you will either download the model to S3 and leave this True, or set this to false and fill in the HuggingFace Model ID and Token if necessary.\n",
    "USE_LOCAL_MODEL_FROM_S3 = True\n",
    "\n",
    "\n",
    "if USE_LOCAL_MODEL_FROM_S3 == True:\n",
    "    os.environ['use_local']=\"true\"\n",
    "    #the default path set here is for workshop environments. \n",
    "    #If using this outside of a hosted workshop, you will need to set this to wherever you downloaded your model.\n",
    "    #Ignore the model_id and hf_token fields, they are simply being cleared here to avoid conflicts with subsequent runs.\n",
    "    os.environ['model_id']=\"\"\n",
    "    os.environ['hf_token']=\"\"\n",
    "    os.environ['base_model_s3_path']=f\"s3://{sess.default_bucket()}/sagemaker/models/base/llama3_1_8b_instruct/\"\n",
    "\n",
    "else:\n",
    "    os.environ['use_local']=\"false\"\n",
    "    # Model_ID - set this to the HuggingFace Model ID you want to load.\n",
    "    os.environ['model_id']=\"meta-llama/Meta-Llama-3.1-8B-Instruct\"    \n",
    "    # HF_Token - use your HuggingFace Token here to access gated models. Llama-3-8B-Instruct is a gated model.\n",
    "    os.environ['hf_token']=\"<your-hf-token>\"\n",
    "    #ignore this env variable for remote mode\n",
    "    os.environ['base_model_s3_path']=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this variable and set the value to your MLFlow Tracking Server ARN to activate MLFLow experiment tracking\n",
    "# os.environ['mlflow_tracking_server_arn']=\"<mlflow-server-arn>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create and prepare the dataset\n",
    "\n",
    "We will use SAMSum dataset which consists of approximately 16,000 messenger-like conversations designed for the task of abstractive summarization. Created by linguists fluent in English, the dataset captures real-life dialogue styles and includes informal, semi-formal, and formal conversations, often containing slang and emoticons. Each conversation is paired with a concise human-written summary that encapsulates the main points discussed. The dataset is divided into training, validation, and test splits, with 14,732 examples in the training set, 818 in validation, and 819 in the test set, making it a valuable resource for research in dialogue summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert dataset to summarization messages    \n",
    "def create_summarization_prompts(data_point):\n",
    "    full_prompt =f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "                    You are an AI assistant trained to summarize conversations. Provide a concise summary of the dialogue, capturing the key points and overall context.\n",
    "                    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "                    Summarize the following conversation:\n",
    "\n",
    "                    {data_point[\"dialogue\"]}\n",
    "                    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "                    Here's a concise summary of the conversation:\n",
    "\n",
    "                    {data_point[\"summary\"]}\n",
    "                    <|eot_id|>\"\"\"\n",
    "    return {\"prompt\": full_prompt}\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(dataset_name, trust_remote_code=True)\n",
    "\n",
    "# Add system message to each conversation\n",
    "columns_to_remove = list(dataset[\"train\"].features)\n",
    "\n",
    "dataset = dataset.map(\n",
    "    create_summarization_prompts,\n",
    "    remove_columns=columns_to_remove,\n",
    "    batched=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Review dataset\n",
    "dataset, dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we processed the datasets we are going to use the [FileSystem integration](https://huggingface.co/docs/datasets/filesystems) to upload our dataset to S3. We are using the `sess.default_bucket()`, adjust this if you want to store the dataset in a different S3 bucket. We will use the S3 path later in our training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save train_dataset to s3 using our SageMaker session\n",
    "input_path = f's3://{sess.default_bucket()}/datasets/llama3'\n",
    "\n",
    "# Save datasets to s3\n",
    "# We will fine tune only with 20 records due to limited compute resource for the workshop\n",
    "dataset[\"train\"].select(range(100)).to_json(f\"{input_path}/train/dataset.json\", orient=\"records\")\n",
    "train_dataset_s3_path = f\"{input_path}/train/dataset.json\"\n",
    "dataset[\"test\"].select(range(20)).to_json(f\"{input_path}/test/dataset.json\", orient=\"records\")\n",
    "test_dataset_s3_path = f\"{input_path}/test/dataset.json\"\n",
    "\n",
    "print(f\"Training data uploaded to:\")\n",
    "print(train_dataset_s3_path)\n",
    "print(test_dataset_s3_path)\n",
    "print(f\"\\nYou can view the uploaded dataset in the console here: \\nhttps://s3.console.aws.amazon.com/s3/buckets/{sess.default_bucket()}/?region={sess.boto_region_name}&prefix={input_path.split('/', 3)[-1]}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOU CAN USE THIS CONFIGURATION FOR A TRAINING RUN ON THE COMPLETE DATASET. \n",
    "# THIS WILL NOT WORK IN A WORKSHOP ENVIRONMENT DUE TO RESOURCE CONSTRAINTS (ml.p3.2xlarge).\n",
    "\n",
    "# save train_dataset to s3 using our SageMaker session\n",
    "#input_path = f's3://{sess.default_bucket()}/datasets/llama3'\n",
    "\n",
    "# Save datasets to s3\n",
    "# We will fine tune only with 20 records due to limited compute resource for the workshop\n",
    "#dataset[\"train\"].to_json(f\"{input_path}/train/dataset.json\", orient=\"records\")\n",
    "#train_dataset_s3_path = f\"{input_path}/train/dataset.json\"\n",
    "#dataset[\"test\"].to_json(f\"{input_path}/test/dataset.json\", orient=\"records\")\n",
    "#test_dataset_s3_path = f\"{input_path}/test/dataset.json\"\n",
    "\n",
    "#print(f\"Training data uploaded to:\")\n",
    "#print(train_dataset_s3_path)\n",
    "#print(test_dataset_s3_path)\n",
    "#print(f\"https://s3.console.aws.amazon.com/s3/buckets/{sess.default_bucket()}/?region={sess.boto_region_name}&prefix={input_path.split('/', 3)[-1]}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure input length\n",
    "\n",
    "While passing in a dataset to the LLM for fine-tuning, it's important to ensure that the inputs are all of a uniform length. To achieve this, we first visualize the distribution of the input token lengths (or alternatively, firectly find the max length). Based on these results, we identify the maximum input token length, and utilize \"padding\" to ensure all the inputs are of the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_data_lengths(tokenized_train_dataset, tokenized_validation_dataset):\n",
    "    lengths1 = [len(x[\"prompt\"]) for x in tokenized_train_dataset]\n",
    "    lengths2 = [len(x[\"prompt\"]) for x in tokenized_validation_dataset]\n",
    "    lengths = lengths1 + lengths2\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.hist(lengths, bins=20, alpha=0.7, color=\"blue\")\n",
    "    plt.xlabel(\"prompt lengths\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Distribution of lengths of input_ids\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_data_lengths(dataset[\"train\"], dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine-tune Llama 3.1 on Amazon SageMaker\n",
    "\n",
    "We are now ready to fine-tune our model. We will use the [SFTTrainer](https://huggingface.co/docs/trl/sft_trainer) from `trl` to fine-tune our model. The `SFTTrainer` makes it straightfoward to supervise fine-tune open LLMs. The `SFTTrainer` is a subclass of the `Trainer` from the `transformers`. We prepared a script [launch_fsdp_qlora.py](../scripts/launch_fsdp_qlora.py) which will loads the dataset from disk, prepare the model, tokenizer and start the training. It usees the [SFTTrainer](https://huggingface.co/docs/trl/sft_trainer) from `trl` to fine-tune our model. \n",
    "\n",
    "For configuration we use `TrlParser`, that allows us to provide hyperparameters in a yaml file. This `yaml` will be uploaded and provided to Amazon SageMaker similar to our datasets. Below is the config file for fine-tuning Llama 3.1 8B on ml.p3.2xlarge 16GB GPUs. We are saving the config file as `args.yaml` and upload it to S3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cat > ./args.yaml <<EOF\n",
    "hf_token: \"${hf_token}\"                         # Use HF token to login into Hugging Face to access the Llama 3.1 8b model\n",
    "model_id: \"${model_id}\"                         # Hugging Face model id\n",
    "use_local: \"${use_local}\"\n",
    "\n",
    "max_seq_length: 2048  #512 # 2048               # max sequence length for model and packing of the dataset\n",
    "# sagemaker specific parameters\n",
    "train_dataset_path: \"/opt/ml/input/data/train/\" # path to where SageMaker saves train dataset\n",
    "test_dataset_path: \"/opt/ml/input/data/test/\"   # path to where SageMaker saves test dataset\n",
    "base_model_s3_path: \"/opt/ml/input/data/basemodel/\"\n",
    "#tokenizer_s3_path: \"/opt/ml/input/data/tokenizer/\"\n",
    "ml_tracking_server_arn: \"${mlflow_tracking_server_arn}\"\n",
    "\n",
    "output_dir: \"/opt/ml/model/llama3.1/adapters/sum\"         # path to where SageMaker will upload the model \n",
    "# training parameters\n",
    "report_to: \"mlflow\"                    # report metrics to tensorboard\n",
    "learning_rate: 0.0002                  # learning rate 2e-4\n",
    "lr_scheduler_type: \"constant\"          # learning rate scheduler\n",
    "num_train_epochs: 1                    # number of training epochs\n",
    "per_device_train_batch_size: 1         # batch size per device during training\n",
    "per_device_eval_batch_size: 1          # batch size for evaluation\n",
    "gradient_accumulation_steps: 1         # number of steps before performing a backward/update pass\n",
    "optim: adamw_torch                     # use torch adamw optimizer\n",
    "logging_steps: 10                      # log every 10 steps\n",
    "save_strategy: epoch                   # save checkpoint every epoch\n",
    "eval_strategy: epoch                   # evaluate every epoch\n",
    "max_grad_norm: 0.3                     # max gradient norm\n",
    "warmup_ratio: 0.03                     # warmup ratio\n",
    "bf16: false                            # use bfloat16 precision\n",
    "tf32: false                            # use tf32 precision\n",
    "fp16: true\n",
    "gradient_checkpointing: true           # use gradient checkpointing to save memory\n",
    "# FSDP parameters: https://huggingface.co/docs/transformers/main/en/fsdp\n",
    "fsdp: \"full_shard auto_wrap offload\"   # remove offload if enough GPU memory\n",
    "fsdp_config:\n",
    "  backward_prefetch: \"backward_pre\"\n",
    "  forward_prefetch: \"false\"\n",
    "  use_orig_params: \"false\"\n",
    "\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets upload the config file to S3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "# upload the model yaml file to s3\n",
    "model_yaml = \"args.yaml\"\n",
    "train_config_s3_path = S3Uploader.upload(local_path=model_yaml, desired_s3_uri=f\"{input_path}/config\")\n",
    "\n",
    "print(f\"Training config uploaded to:\")\n",
    "print(train_config_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune LoRA adapter\n",
    "\n",
    "Below estimtor will train the model with QLoRA and will save the LoRA adapter in S3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create SageMaker PyTorch Estimator\n",
    "\n",
    "# define Training Job Name \n",
    "job_name = f'llama3-1-8b-finetune'\n",
    "\n",
    "pytorch_estimator = PyTorch(\n",
    "    entry_point= 'launch_fsdp_qlora.py',\n",
    "    source_dir=\"./scripts\",\n",
    "    job_name=job_name,\n",
    "    base_job_name=job_name,\n",
    "    max_run=5800,\n",
    "    role=role,\n",
    "    framework_version=\"2.2.0\",\n",
    "    py_version=\"py310\",\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.p3.2xlarge\",\n",
    "    sagemaker_session=sess,\n",
    "    volume_size=50,\n",
    "    disable_output_compression=True,\n",
    "    keep_alive_period_in_seconds=1800,\n",
    "    distribution={\"torch_distributed\": {\"enabled\": True}},\n",
    "    hyperparameters={\n",
    "        \"config\": \"/opt/ml/input/data/config/args.yaml\" # path to TRL config which was uploaded to s3\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: When using QLoRA, we only train adapters and not the full model. The [launch_fsdp_qlora.py](../scripts/fsdp/run_fsdp_qlora.py) saves the `adapter` at the end of the training to Amazon SageMaker S3 bucket (sagemaker-<region name>-<account_id>)._\n",
    "\n",
    "We can now start our training job, with the `.fit()` method passing our S3 path to the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {\n",
    "  'train': train_dataset_s3_path,\n",
    "  'test': test_dataset_s3_path,\n",
    "  'config': train_config_s3_path\n",
    "  }\n",
    "\n",
    "if(os.environ[\"use_local\"].lower()==\"true\"):\n",
    "    data.update({'basemodel':os.environ['base_model_s3_path']})    \n",
    " \n",
    "# Check input channels configured \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "pytorch_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fine the job name of the last run or you can browse the console\n",
    "latest_run_job_name=pytorch_estimator.latest_training_job.job_name\n",
    "latest_run_job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find S3 path for the last job that ran successfully. You can find this from the SageMaker console \n",
    "\n",
    "# *** Get a job name from the AWS console for the last training run or from the above cell\n",
    "job_name = latest_run_job_name\n",
    "\n",
    "def get_s3_path_from_job_name(job_name):\n",
    "    # Create a Boto3 SageMaker client\n",
    "    sagemaker_client = boto3.client('sagemaker')\n",
    "    \n",
    "    # Describe the training job\n",
    "    response = sagemaker_client.describe_training_job(TrainingJobName=job_name)\n",
    "    \n",
    "    # Extract the model artifacts S3 path\n",
    "    model_artifacts_s3_path = response['ModelArtifacts']['S3ModelArtifacts']\n",
    "    \n",
    "    # Extract the output path (this is the general output location)\n",
    "    output_path = response['OutputDataConfig']['S3OutputPath']\n",
    "    \n",
    "    return model_artifacts_s3_path, output_path\n",
    "\n",
    "\n",
    "model_artifacts, output_path = get_s3_path_from_job_name(job_name)\n",
    "\n",
    "print(f\"Model artifacts S3 path: {model_artifacts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Point to the directory where we have the adapter saved \n",
    "\n",
    "adapter_dir_path=f\"{model_artifacts}/llama3.1/adapters/sum/\"\n",
    "\n",
    "adapter_serving_dir_path=f\"{model_artifacts}/llama3.1/\"\n",
    "\n",
    "print(f'\\nAdapter S3 Dir path:{adapter_dir_path} \\n')\n",
    "\n",
    "print(f'\\nServing S3 Dir path:{adapter_serving_dir_path} \\n')\n",
    "\n",
    "!aws s3 ls {adapter_dir_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming you already have this environment variable set\n",
    "base_model_s3_path = os.environ['base_model_s3_path'] if os.environ['use_local'].lower() == 'true' else os.environ['model_id']\n",
    "\n",
    "# Store the variables required for the next notebook \n",
    "%store base_model_s3_path\n",
    "%store adapter_serving_dir_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Step - Use register_model_adapter.ipynb notebook to register the adapter to the SageMaker model registry "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Step - Merge base model with fine-tuned adapter in fp16 and Test Inference \n",
    "\n",
    "Following Steps are taken by the next estimator:\n",
    "1. Load base model in fp16 precision\n",
    "2. Convert adapter saved in previous step from fp32 to fp16\n",
    "3. Merge the model\n",
    "4. Run inference both on base model and merged model for comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create SageMaker PyTorch Estimator\n",
    "\n",
    "# Define Training Job Name \n",
    "job_name = f'llama3-1-8b-merge-adapter'\n",
    "\n",
    "pytorch_estimator_adapter = PyTorch(\n",
    "    entry_point= 'merge_model_adapter.py',\n",
    "    source_dir=\"./scripts\",\n",
    "    job_name=job_name,\n",
    "    base_job_name=job_name,\n",
    "    max_run=5800,\n",
    "    role=role,\n",
    "    framework_version=\"2.2.0\",\n",
    "    py_version=\"py310\",\n",
    "    instance_count=1,\n",
    "    volume_size=50,\n",
    "    instance_type=\"ml.p3.2xlarge\",\n",
    "    sagemaker_session=sess,\n",
    "    disable_output_compression=True,\n",
    "    keep_alive_period_in_seconds=1800,\n",
    "    hyperparameters={\n",
    "        \"model_id\": os.environ['model_id'],  # Hugging Face model id\n",
    "        \"hf_token\": os.environ['hf_token'],\n",
    "        \"dataset_name\":dataset_name,\n",
    "        \"use_local\": os.environ['use_local']\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 ls {adapter_dir_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {\n",
    "  'adapter': adapter_dir_path,\n",
    "  'testdata': test_dataset_s3_path \n",
    "  }\n",
    "\n",
    "if(os.environ[\"use_local\"].lower()==\"true\"):\n",
    "    data.update({'basemodel':os.environ['base_model_s3_path']})\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "pytorch_estimator_adapter.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
